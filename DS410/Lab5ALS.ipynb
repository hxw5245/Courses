{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Spring 2022\n",
    "# Instructor: Professor John Yen\n",
    "# TA: Rupesh Prajapati \n",
    "# LAs: Lily Jakielaszek and Cayla Shan Pun\n",
    "# Lab 5: Movie Recommendations Using Alternative Least Square\n",
    "## The goals of this lab are for you to be able to\n",
    "### - Use Alternating Least Squares (ALS) for recommending movies based on reviews of users\n",
    "### - Be able to understand the raionale for splitting data into training, validation, and testing.\n",
    "### - Be able to use MLlib to implement an ALS-based movie recommendation system.\n",
    "### - Be able to use RDD transformations to calculate training error, validation error, and prediction error of a model.\n",
    "### - Be able to tune hyper-parameters of the ALS model using a small dataset (in local mode)\n",
    "### - Be able to store the results of evaluating hyper-parameters\n",
    "### - Be able to select best hyper-parameters and evaluate the chosen model with testing data\n",
    "### - Be able to improve the efficiency of iterative processing using persist (and unpersist, if applicable)\n",
    "### - Be able to use CheckPoint to improve the efficiency of iterative processing using Spark.\n",
    "### - Be able to debug (in local mode) using Restart Kernel if needed.\n",
    "\n",
    "## Exercises: \n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 10 points\n",
    "- Exercise 3: 10 points\n",
    "- Exercise 4: 10 points\n",
    "- Exercise 5: 10 points\n",
    "- Exercise 6: 10 points\n",
    "- Exercise 7: 25 points\n",
    "- Exercise 8: 15 points\n",
    "- Exercise 9: 10 points\n",
    "## Total Points: 105 points\n",
    "\n",
    "# Due: midnight, Feb 13, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first thing we need to do in each Jupyter Notebook running pyspark is to import pyspark first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we import pyspark, we need to import \"SparkContext\".  Every spark program needs a SparkContext object\n",
    "### In order to use Spark SQL on DataFrames, we also need to import SparkSession from PySpark.SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We then create a Spark Session variable (rather than Spark Context) in order to use DataFrame. \n",
    "- Note: We temporarily use \"local\" as the parameter for master in this notebook so that we can test it in ICDS Roar.  However, we need to change \"local\" to \"Yarn\" before we submit it to XSEDE to run in cluster mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SparkSession.builder.master(\"local\").appName(\"Lab5 Recommendation Systems\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setCheckpointDir(\"~/scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (5 points) (a) Add your name below AND (b) replace the path below with the path of your home directory.\n",
    "## Answer for Exercise 1\n",
    "- a: Your Name: Haichen Wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_DF = ss.read.csv(\"/storage/home/hxw5245/Lab4/ratings_2.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- MovieID: integer (nullable = true)\n",
      " |-- Rating: double (nullable = true)\n",
      " |-- RatingID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2_DF = ratings_DF.select(\"UserID\",\"MovieID\",\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(UserID=1, MovieID=31, Rating=2.5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings2_DF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(UserID=1, MovieID=31, Rating=2.5),\n",
       " Row(UserID=1, MovieID=1029, Rating=3.0),\n",
       " Row(UserID=1, MovieID=1061, Rating=3.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings2_RDD = ratings2_DF.rdd\n",
    "ratings2_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.1 Split Data into Three Sets: Training Data, Evaluatiion Data, and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (10 points)\n",
    "## Complete the code below to split `ratings2_RDD` into three groups: 60% training, 20% validation, and 20% testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = ratings2_RDD.randomSplit([3, 1, 1], 521)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input (UserID, MovieID) for validation and for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_RDD = training_RDD.map(lambda x: (x[0], x[1]) )\n",
    "validation_input_RDD = validation_RDD.map(lambda x: (x[0], x[1]) ) \n",
    "testing_input_RDD = test_RDD.map(lambda x: (x[0], x[1]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.2 Construct a Movie Recommendation Model \n",
    "## using ALS (from `PySpark.MLlib.recommendation` module) and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ALS.train(training_RDD, 4, seed=41, iterations=30, lambda_=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (10 points)\n",
    "## Complete the code below to transform the model output of training data into the format of `( (<UserID> <MovieID>), <predictedRating> )` so that we can later join it with actual Rating for computing RMS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prediction_RDD = model.predictAll(training_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((599, 69069), 3.925916741285466),\n",
       " ((270, 81132), 3.9015987051642433),\n",
       " ((390, 667), 2.33796273224989)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_prediction_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_evaluation_RDD = training_RDD.map(lambda y: ( (y[0], y[1]), y[2]) ).join(training_prediction_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 31), (2.5, 2.59967650141975)),\n",
       " ((1, 1029), (3.0, 2.697573645098246)),\n",
       " ((1, 1061), (3.0, 2.7537082775211985))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_evaluation_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = math.sqrt(training_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6498690270748771\n"
     ]
    }
   ],
   "source": [
    "print(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prediction_RDD = model.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((402, 69069), 3.2310862470123247),\n",
       " ((624, 69069), 2.210174197884259),\n",
       " ((213, 44828), 2.174816531472955)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_prediction_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (10 points)\n",
    "## Complete the code below to join `validation_RDD` (after transforming it into the same key value pair format of Exercise 3, and `validation_prediction_RDD` to prepare for RMS error calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_evaluation_RDD = validation_RDD.map(lambda y: ( (y[0], y[1]), y[2] )).join(validation_prediction_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((1, 1129), (2.0, 2.3653280077890746)),\n",
       " ((1, 1953), (4.0, 2.789249674020755)),\n",
       " ((1, 2105), (4.0, 2.1729105277828893))]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_evaluation_RDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (10 points)\n",
    "## Complete the code below to calculate RMS error for validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9400613190737446\n"
     ]
    }
   ],
   "source": [
    "print(validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (10 points)\n",
    "## Complete the code below that computes the RMS errors of training data and validation data for three values of rank (k = 4, 7, and 10) in ALS model (for regularization hyperparameter = 0.1, number of iterations 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " rank is  4 , Training Error =  0.6495805650417658 , Validation Error = 0.9337770394103238\n",
      " rank is  7 , Training Error =  0.573574429116544 , Validation Error = 0.9473097548982903\n",
      " rank is  10 , Training Error =  0.5265304336062925 , Validation Error = 0.9455620990645692\n"
     ]
    }
   ],
   "source": [
    "index =0 \n",
    "# initialize lowest_error\n",
    "lowest_validation_error = float('inf')\n",
    "lowest_training_error = float('inf')\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "iterations = 30\n",
    "regularization = 0.1\n",
    "rank_list = [4, 7, 10]\n",
    "for k in rank_list:\n",
    "    seed = 43\n",
    "    # Construct a recommendation model using a set of hyper-parameter values and training data\n",
    "    model = ALS.train(training_RDD, k, seed=seed, iterations=iterations, lambda_=regularization)\n",
    "    training_prediction_RDD = model.predictAll(training_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] ) )\n",
    "    training_evaluation_RDD = training_RDD.map(lambda y: ( (y[0], y[1]), y[2] ) ).join(training_prediction_RDD)\n",
    "    training_error = math.sqrt(training_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())\n",
    "    # Evaluate the model using evalution data\n",
    "    # map the output into ( (userID, movieID), rating ) so that we can join with actual evaluation data\n",
    "    # using (userID, movieID) as keys.\n",
    "    validation_prediction_RDD= model.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] )   )\n",
    "    validation_evaluation_RDD = validation_RDD.map(lambda y: ( (y[0], y[1]), y[2] )  ).join(validation_prediction_RDD)\n",
    "    # Calculate RMS error between the actual rating and predicted rating for (userID, movieID) pairs in validation dataset\n",
    "    validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())\n",
    "    # Save the error as a row in a pandas DataFrame\n",
    "    # hyperparams_eval_df.loc[index] = [k, regularization, iterations, training_error, validation_error, float('inf')]\n",
    "    index = index + 1\n",
    "    print(' rank is ', k, ', Training Error = ', training_error, ', Validation Error =', validation_error)\n",
    "    # Check whether the current error is the lowest\n",
    "    if validation_error < lowest_validation_error:\n",
    "        best_k = k\n",
    "        best_validation_index = index - 1 \n",
    "        lowest_validation_error = validation_error\n",
    "    if training_error < lowest_training_error:\n",
    "        best_training_k = k\n",
    "        best_training_index = index - 1\n",
    "        lowest_training_error = training_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.3 Hyperparameter Tuning\n",
    "## Iterate through all possible combination of a set of values for three hyperparameters for ALS Recommendation Model:\n",
    "- rank (k)\n",
    "- regularization\n",
    "- iterations \n",
    "## Each hyperparameter value combination is used to construct an ALS recommendation model using training data, but evaluate using Evaluation Data\n",
    "## The evaluation results are saved in a Pandas DataFrame \n",
    "``\n",
    "hyperparams_eval_df\n",
    "``\n",
    "## The best hyperprameter value combination is stored in 4 variables\n",
    "``\n",
    "best_k, best_regularization, best_iterations, and lowest_validation_error\n",
    "``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (25 points) \n",
    "## Complete the code below to iterate through the following set of hyperparameters to create and evaluate ALS recommendation models:\n",
    "- iterations : 15, 30\n",
    "- regularization: 0.1, 0.2\n",
    "- rank: 4, 7, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best rank k is  7 , regularization =  0.2 , iterations =  30 . Validation Error = 0.9181386561528002\n"
     ]
    }
   ],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['k', 'regularization', 'iterations', 'validation RMS', 'testing RMS'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "lowest_validation_error = float('inf')\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "iterations_list = [15, 30]\n",
    "regularization_list = [0.1, 0.2]\n",
    "rank_list = [4, 7, 10]\n",
    "for k in rank_list:\n",
    "    for regularization in regularization_list:\n",
    "        for iterations in iterations_list:\n",
    "            seed = 37\n",
    "            # Construct a recommendation model using a set of hyper-parameter values and training data\n",
    "            model = ALS.train(training_RDD, k, seed=seed, iterations=iterations, lambda_=regularization)\n",
    "            # Evaluate the model using evalution data\n",
    "            # map the output into ( (userID, movieID), rating ) so that we can join with actual evaluation data\n",
    "            # using (userID, movieID) as keys.\n",
    "            validation_prediction_RDD= model.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2])   )\n",
    "            validation_evaluation_RDD = validation_RDD.map(lambda y: ( (y[0], y[1]), y[2]) ).join(validation_prediction_RDD)\n",
    "            # Calculate RMS error between the actual rating and predicted rating for (userID, movieID) pairs in validation dataset\n",
    "            validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())\n",
    "            # Save the error as a row in a pandas DataFrame\n",
    "            hyperparams_eval_df.loc[index] = [k, regularization, iterations, validation_error, float('inf')]\n",
    "            index = index + 1\n",
    "            # Check whether the current error is the lowest\n",
    "            if validation_error < lowest_validation_error:\n",
    "                best_k = k\n",
    "                best_regularization = regularization\n",
    "                best_iterations = iterations\n",
    "                best_index = index - 1\n",
    "                lowest_validation_error = validation_error\n",
    "print('The best rank k is ', best_k, ', regularization = ', best_regularization, ', iterations = ',\\\n",
    "      best_iterations, '. Validation Error =', lowest_validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Testing Data to Evaluate the Model built using the Best Hyperparameters                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.4 Evaluate the best hyperparameter combination using testing data\n",
    "## If the error between rating prediction and actual rating for (userID, movie ID) pairs in the training data is comparable to the error of validation data, our model passes the test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (15 points)\n",
    "- (a) Complete the code below to evaluate the best hyperparameter combinations using testing data. (10 point)\n",
    "- (b) Does your model pass the test of testing data? Explain your answer in the Markdown cell below. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Testing Error for rank k = 7  regularization =  0.2 , iterations =  30  is :  0.9181095896148572\n"
     ]
    }
   ],
   "source": [
    "seed = 37\n",
    "model = ALS.train(training_RDD, best_k, seed=seed, iterations=best_iterations, lambda_=best_regularization)\n",
    "testing_prediction_RDD=model.predictAll(testing_input_RDD).map(lambda x: ((x[0], x[1]), x[2]))\n",
    "testing_evaluation_RDD= test_RDD.map(lambda x: ((x[0], x[1]), x[2])).join(testing_prediction_RDD)\n",
    "testing_error = math.sqrt(testing_evaluation_RDD.map(lambda x: (x[1][0] - x[1][1])**2).mean())\n",
    "print('The Testing Error for rank k =', best_k, ' regularization = ', best_regularization, ', iterations = ', \\\n",
    "      best_iterations, ' is : ', testing_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to Exercise 8: \n",
    "- (b) Yes, the model pass the test, both best rank k is 7, regularaization is 0.2, iterations is 30, and the testing error is close to the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_k, best_regularization, best_iterations, lowest_validation_error, testing_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema3= StructType([ StructField(\"k\", FloatType(), True), \\\n",
    "                      StructField(\"regularization\", FloatType(), True ), \\\n",
    "                      StructField(\"iterations\", FloatType(), True), \\\n",
    "                      StructField(\"Validation RMS\", FloatType(), True), \\\n",
    "                      StructField(\"Testing RMS\", FloatType(), True) \\\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the pandas DataFrame that stores validation errors of all hyperparameters and the testing error for the best model to Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParams_Tuning_DF = ss.createDataFrame(hyperparams_eval_df, schema3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9 (10 points)\n",
    "## Modify the output path so that your output results can be saved in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/hxw5245/Lab5/Lab5ALSHyperParamsTuning\"\n",
    "HyperParams_Tuning_DF.rdd.saveAsTextFile(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
