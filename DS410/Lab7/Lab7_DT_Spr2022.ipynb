{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Spring 2022\n",
    "# Lab 7 Decision Tree Learning Using ML Pipeline, Visualization, and Hyperparameter Tuning\n",
    "\n",
    "# Instructor: Professor John Yen\n",
    "# TA: Rupesh Prajapati \n",
    "# LAs: Lily Jakielaszek and Cayla Shan Pun\n",
    "\n",
    "## The goals of this lab are for you to be able to\n",
    "- Understand the function of the different steps/stages involved in Spark ML pipeline\n",
    "- Be able to construct a decision tree using Spark ML machine learning module\n",
    "- Be able to generate a visualization of Decision Trees\n",
    "- Be able to use Spark ML pipeline to perform automated hyper-parameter tuning for Decision Trees \n",
    "- Be able to apply .persist() to suitable DataFrames and evaluate its impact on computation time.\n",
    "\n",
    "## The data set used in this lab is a Breast Cancer diagnosis dataset.\n",
    "\n",
    "## Submit the following items for Lab 7 (DT)\n",
    "- Completed Jupyter Notebook of Lab 7 (in HTML format)\n",
    "- A .py file (e.g., Lab7DT.py) and logfile for running ONLY part 5 of this in cluster \n",
    "- A .py file (e.g., Lab7DT_P.py) and logfile for running ONLY part 5 of this in cluster, with persist() on two DataFrames.\n",
    "- The output file that contains the best hyperparameters\n",
    "\n",
    "## Total Number of Exercises: 100\n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 5 points\n",
    "- Exercise 3: 10 points  \n",
    "- Exercise 4: 10 points \n",
    "- Exercise 5: 15 points\n",
    "- Exercise 6: 15 points\n",
    "- Exercise 7: 20 points\n",
    "- Exercise 8: 20 points\n",
    "## Total Points: 100 points\n",
    "\n",
    "# Due: midnight, Feb 27, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and set up the Python files for this Lab\n",
    "1. Create a \"Lab7DT\" directory in the work directory of your ICDS-ROAR home directory.\n",
    "2. If you have not done so, copy or upload this file to the \"Lab7DT\" directory.\n",
    "3. Create a subdirectory under \"Lab7DT\" called \"decision_tree_plot\" (named the directory EXACTLY this way).\n",
    "4. Upload the following three files in Module 8 from Canvas to the decision_tree_plot directory\n",
    "- decision_tree_parser.py\n",
    "- decision_tree_plot.py\n",
    "- tree_template.jinjia2\n",
    "\n",
    "# Follow the instructions below and execute the PySpark code cell by cell below. Make modifications as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notice that we use PySpark SQL module to import SparkSession because ML works with SparkSession\n",
    "## Notice also the different methods imported from ML and three submodules of ML: classification, feature, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following two lines import relevant functions from the two python files you uploaded into the decision_tree_plot subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decision_tree_plot.decision_tree_parser import decision_tree_parse\n",
    "from decision_tree_plot.decision_tree_plot import plot_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This lab runs Spark in the local mode.\n",
    "## Notice we are creating a SparkSession, not a SparkContext, when we use ML pipeline.\n",
    "## The \"getOrCreate()\" method means we can re-evaluate this without a need to \"stop the current SparkSession\" first (unlike SparkContext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SparkSession.builder.master(\"local\").appName(\"lab 7 DT\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: (5 points) Enter your name below:\n",
    "- My Name: Haichen Wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As we have seen in Lab 4, SparkSession offers a way to read a CSV/text file with the capability to interpret the first row as being the header and infer the type of different columns based on their values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: (5 points) Complete the following path with the path for your home directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ss.read.csv(\"/storage/home/hxw5245/Lab7DT/breast-cancer-wisconsin.data.txt\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Feature Transformation Using DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- clump_thickness: integer (nullable = true)\n",
      " |-- unif_cell_size: integer (nullable = true)\n",
      " |-- unif_cell_shape: integer (nullable = true)\n",
      " |-- marg_adhesion: integer (nullable = true)\n",
      " |-- single_epith_cell_size: integer (nullable = true)\n",
      " |-- bare_nuclei: string (nullable = true)\n",
      " |-- bland_chrom: integer (nullable = true)\n",
      " |-- norm_nucleoli: integer (nullable = true)\n",
      " |-- mitoses: integer (nullable = true)\n",
      " |-- class: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|     id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "|1000025|              5|             1|              1|            1|                     2|          1|          3|            1|      1|    2|\n",
      "|1002945|              5|             4|              4|            5|                     7|         10|          3|            2|      1|    2|\n",
      "|1015425|              3|             1|              1|            1|                     2|          2|          3|            1|      1|    2|\n",
      "|1016277|              6|             8|              8|            1|                     3|          4|          3|            7|      1|    2|\n",
      "|1017023|              4|             1|              1|            3|                     2|          1|          3|            1|      1|    2|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|class|count|\n",
      "+-----+-----+\n",
      "|    4|  241|\n",
      "|    2|  458|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "class_count = data.groupBy(col(\"class\")).count()\n",
    "class_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnIndexer = StringIndexer(inputCol=\"bare_nuclei\", outputCol=\"bare_nuclei_index\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=StringIndexer_933a948fc9b1, handleInvalid=error"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bnIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = bnIndexer.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+-----------------+\n",
      "|     id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|bare_nuclei_index|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+-----------------+\n",
      "|1000025|              5|             1|              1|            1|                     2|          1|          3|            1|      1|    2|              0.0|\n",
      "|1002945|              5|             4|              4|            5|                     7|         10|          3|            2|      1|    2|              1.0|\n",
      "|1015425|              3|             1|              1|            1|                     2|          2|          3|            1|      1|    2|              2.0|\n",
      "|1016277|              6|             8|              8|            1|                     3|          4|          3|            7|      1|    2|              6.0|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+-----------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIndexer= StringIndexer(inputCol=\"class\", outputCol=\"indexedLabel\").fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=StringIndexer_afb7331903a6, handleInvalid=error"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed2_data = labelIndexer.transform(transformed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+-----------------+------------+\n",
      "|     id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|bare_nuclei_index|indexedLabel|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+-----------------+------------+\n",
      "|1000025|              5|             1|              1|            1|                     2|          1|          3|            1|      1|    2|              0.0|         0.0|\n",
      "|1002945|              5|             4|              4|            5|                     7|         10|          3|            2|      1|    2|              1.0|         0.0|\n",
      "|1015425|              3|             1|              1|            1|                     2|          2|          3|            1|      1|    2|              2.0|         0.0|\n",
      "|1016277|              6|             8|              8|            1|                     3|          4|          3|            7|      1|    2|              6.0|         0.0|\n",
      "+-------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+-----------------+------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed2_data.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = ['clump_thickness', 'unif_cell_size', 'unif_cell_shape', 'marg_adhesion', \\\n",
    "                  'single_epith_cell_size', 'bare_nuclei_index', 'bland_chrom', 'norm_nucleoli', 'mitoses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=input_features, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorAssembler_033c3ec3996c"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed3_data = assembler.transform(transformed2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|            features|indexedLabel|\n",
      "+--------------------+------------+\n",
      "|[5.0,1.0,1.0,1.0,...|         0.0|\n",
      "|[5.0,4.0,4.0,5.0,...|         0.0|\n",
      "|[3.0,1.0,1.0,1.0,...|         0.0|\n",
      "|[6.0,8.0,8.0,1.0,...|         0.0|\n",
      "|[4.0,1.0,1.0,3.0,...|         0.0|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected_transformed3_data = transformed3_data.select(\"features\",'indexedLabel')\n",
    "selected_transformed3_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Decision Tree Learning and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomSplit is a method for DataFrame that split data in the DataFrame into two subsets, one for training, the other for testing, using a number as the seed for random number generator.\n",
    "## If you want to generate a different split, you can use a different seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData3, testData3= transformed3_data.randomSplit([0.75, 0.25], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"indexedLabel\", maxDepth=6, minInstancesPerNode=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier_e6a043c1f024"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = dt.fit(trainingData3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_e6a043c1f024, depth=6, numNodes=33, numClasses=2, numFeatures=9"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = dt_model.transform(testData3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+-----------------+------------+--------------------+-------------+--------------------+----------+\n",
      "|    id|clump_thickness|unif_cell_size|unif_cell_shape|marg_adhesion|single_epith_cell_size|bare_nuclei|bland_chrom|norm_nucleoli|mitoses|class|bare_nuclei_index|indexedLabel|            features|rawPrediction|         probability|prediction|\n",
      "+------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+-----------------+------------+--------------------+-------------+--------------------+----------+\n",
      "| 63375|              9|             1|              2|            6|                     4|         10|          7|            7|      2|    4|              1.0|         1.0|[9.0,1.0,2.0,6.0,...|    [0.0,4.0]|           [0.0,1.0]|       1.0|\n",
      "|128059|              1|             1|              1|            1|                     2|          5|          5|            1|      1|    2|              3.0|         0.0|[1.0,1.0,1.0,1.0,...|  [295.0,1.0]|[0.99662162162162...|       0.0|\n",
      "|320675|              3|             3|              5|            2|                     3|         10|          7|            1|      1|    4|              1.0|         1.0|[3.0,3.0,5.0,2.0,...|   [4.0,15.0]|[0.21052631578947...|       1.0|\n",
      "+------+---------------+--------------+---------------+-------------+----------------------+-----------+-----------+-------------+-------+-----+-----------------+------------+--------------------+-------------+--------------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prediction.persist().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+-------------+--------------------+----------+\n",
      "|            features|class|indexedLabel|rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+------------+-------------+--------------------+----------+\n",
      "|[9.0,1.0,2.0,6.0,...|    4|         1.0|    [0.0,4.0]|           [0.0,1.0]|       1.0|\n",
      "|[1.0,1.0,1.0,1.0,...|    2|         0.0|  [295.0,1.0]|[0.99662162162162...|       0.0|\n",
      "|[3.0,3.0,5.0,2.0,...|    4|         1.0|   [4.0,15.0]|[0.21052631578947...|       1.0|\n",
      "|[10.0,8.0,8.0,2.0...|    4|         1.0|  [1.0,126.0]|[0.00787401574803...|       1.0|\n",
      "|[1.0,1.0,1.0,1.0,...|    2|         0.0|  [295.0,1.0]|[0.99662162162162...|       0.0|\n",
      "+--------------------+-----+------------+-------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prediction.select(\"features\",\"class\",\"indexedLabel\", \"rawPrediction\", \"probability\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '4']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelIndexer.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelConverter=IndexToString(inputCol=\"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_prediction = labelConverter.transform(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------+----------+--------------+\n",
      "|            features|class|indexedLabel|prediction|predictedClass|\n",
      "+--------------------+-----+------------+----------+--------------+\n",
      "|[9.0,1.0,2.0,6.0,...|    4|         1.0|       1.0|             4|\n",
      "|[1.0,1.0,1.0,1.0,...|    2|         0.0|       0.0|             2|\n",
      "|[3.0,3.0,5.0,2.0,...|    4|         1.0|       1.0|             4|\n",
      "|[10.0,8.0,8.0,2.0...|    4|         1.0|       1.0|             4|\n",
      "|[1.0,1.0,1.0,1.0,...|    2|         0.0|       0.0|             2|\n",
      "+--------------------+-----+------------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2_prediction.select(\"features\",\"class\",\"indexedLabel\",\"prediction\",\"predictedClass\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.9780273904005113\n"
     ]
    }
   ],
   "source": [
    "f1 = evaluator.evaluate(test_prediction)\n",
    "print(\"f1 score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 DT Learning Using ML Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: (10 points) In the code cell below, fill in a value for maxDepth and a value of minInstancesPerNode. Run the entire sequence of code below to generate a decision tree (using pipeline) and compute f1 measure of the testing data.\n",
    "- Record the f1 measure for the max_depth below  \n",
    "- Recommended value for maxDepth: 2 to 10\n",
    "- Recommended value for minInstancesPerNode: 1 to 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer for Exercise 3: \n",
    "- The f1 measure of testing data for max_detph = 5 and minInstancesPerNode = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData= data.randomSplit([0.75, 0.25], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "dt=DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth=5, minInstancesPerNode=2)\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "pipeline = Pipeline(stages=[labelIndexer, bnIndexer, assembler, dt, labelConverter])\n",
    "model = pipeline.fit(trainingData)\n",
    "test_predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline_5b18e8951d82"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PipelineModel_aba42a53db60"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+--------------+\n",
      "|class|indexedLabel|prediction|predictedClass|\n",
      "+-----+------------+----------+--------------+\n",
      "|    4|         1.0|       1.0|             4|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    4|         1.0|       1.0|             4|\n",
      "|    4|         1.0|       1.0|             4|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    4|         1.0|       1.0|             4|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "|    2|         0.0|       0.0|             2|\n",
      "+-----+------------+----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions.select(\"class\",\"indexedLabel\",\"prediction\",\"predictedClass\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score of testing data: 0.9726090922212769\n"
     ]
    }
   ],
   "source": [
    "f1 = evaluator.evaluate(test_predictions)\n",
    "print(\"f1 score of testing data:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 Decision Tree Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stages[3] of the pipeline is \"dt\" (DecisionTreeClassifier). \n",
    "## model is a DataFrame representing a trained pipeline.\n",
    "## model.stages[3] gives us the Decision Tree model learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: (10 points) \n",
    "- Complete the code below to generate a visualization of the decision tree.\n",
    "- Download the HTML file of the tree and submit it as a part of Lab7 assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_c84b5ce2cbe6, depth=5, numNodes=23, numClasses=2, numFeatures=9\n"
     ]
    }
   ],
   "source": [
    "DTmodel = model.stages[3]\n",
    "print(DTmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=\"./DTmodel_vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tree=decision_tree_parse(DTmodel, ss, model_path)\n",
    "column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(tree, column = column, output_path = '/storage/home/hxw5245/Lab7DT/DTtree2.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 Automated Hyperparameter Tuning for Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: (15 points)  \n",
    "- Complete the code below to perform hyper parameter tuning of Decision Tree (for two parameters: max_depth and minInstancesPerNode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testingData= data.randomSplit([0.75, 0.25], seed=1234)\n",
    "model_path=\"./DTmodel_vis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['max_depth', 'minInstancesPerNode', 'training f1', 'testing f1', 'Best Model'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "highest_testing_f1 = 0\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "max_depth_list = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "minInstancesPerNode_list = [2, 3, 4, 5, 6]\n",
    "assembler = VectorAssembler( inputCols=input_features, outputCol=\"features\")\n",
    "labelConverter = IndexToString(inputCol = \"prediction\", outputCol=\"predictedClass\", labels=labelIndexer.labels)\n",
    "for max_depth in max_depth_list:\n",
    "    for minInsPN in minInstancesPerNode_list:\n",
    "        seed = 37\n",
    "        # Construct a DT model using a set of hyper-parameter values and training data\n",
    "        dt= DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"features\", maxDepth= max_depth, minInstancesPerNode= minInsPN)\n",
    "        pipeline = Pipeline(stages=[labelIndexer, bnIndexer, assembler, dt, labelConverter])\n",
    "        model = pipeline.fit(trainingData)\n",
    "        training_predictions = model.transform(trainingData)\n",
    "        testing_predictions = model.transform(testingData)\n",
    "        evaluator = MulticlassClassificationEvaluator(labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "        training_f1 = evaluator.evaluate(training_predictions)\n",
    "        testing_f1 = evaluator.evaluate(testing_predictions)\n",
    "        # We use 0 as default value of the 'Best Model' column in the Pandas DataFrame.\n",
    "        # The best model will have a value 1000\n",
    "        hyperparams_eval_df.loc[index] = [ max_depth, minInsPN, training_f1, testing_f1, 0]  \n",
    "        index = index +1\n",
    "        if testing_f1 > highest_testing_f1 :\n",
    "            best_max_depth = max_depth\n",
    "            best_minInsPN = minInsPN\n",
    "            best_index = index -1\n",
    "            best_parameters_training_f1 = training_f1\n",
    "            best_DTmodel= model.stages[3]\n",
    "            best_tree = decision_tree_parse(best_DTmodel, ss, model_path)\n",
    "            column = dict( [ (str(idx), i) for idx, i in enumerate(input_features) ])           \n",
    "            highest_testing_f1 = testing_f1\n",
    "print('The best max_depth is ', best_max_depth, ', best minInstancesPerNode = ', \\\n",
    "      best_minInsPN, ', testing f1 = ', highest_testing_f1) \n",
    "# column = dict([(str(idx), i) for idx, i in enumerate(input_features)])\n",
    "plot_trees(best_tree, column = column, output_path = '/storage/home/hxw5245/Lab7DT/bestDTtree2.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_max_depth, best_minInsPN, best_parameters_training_f1, highest_testing_f1, 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema3= StructType([ StructField(\"Max Depth\", FloatType(), True), \\\n",
    "                      StructField(\"MinInstancesPerNode\", FloatType(), True ), \\\n",
    "                      StructField(\"Training f1\", FloatType(), True), \\\n",
    "                      StructField(\"Testing f1\", FloatType(), True), \\\n",
    "                      StructField(\"Best Model\", FloatType(), True) \\\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the pandas DataFrame that stores validation errors of all hyperparameters and the testing error for the best model to Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParams_Tuning_DF = ss.createDataFrame(hyperparams_eval_df, schema3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (15 points)\n",
    "### Complete the path below to save the result of your hyperparameter tuning in a directory.\n",
    "\n",
    "## Notice: Modify the output path before you export this to a .py file for running in cluster mode.  \n",
    "## Notice: Remember to change the output_path directory after each spark-submit in cluster. Otherwise, the spark_submit will NOT run the action (saveAsTextFile) successfully due to being unable to write into an existing directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/hxw5245/Lab7DT/Lab7_B\"\n",
    "HyperParams_Tuning_DF.rdd.saveAsTextFile(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (20 points) \n",
    "## Modify this Notebook to comment out (or remove) Part 1, 2, 3, and 4, and modify it for running spark-submit in cluster mode.  Export it as a .py file (e.g., Lab7DT.py). Record the computation time below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Exercise 7: \n",
    "real: 3m59.904s\n",
    "\n",
    "user: 1m45.409s\n",
    "\n",
    "sys: 0m8.081s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (20 points)\n",
    "## Modify .py file used in Exercise 7 to add `persist()` to two DataFrame for enhanced scalability of the code. (a) Record the computation time below. (b) Compare the computation time with and without persist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer to Exercise 8: \n",
    "\n",
    "(a) real: 2m38.377s\n",
    "\n",
    "user: 1m41.350s\n",
    "\n",
    "sys: 0m7.553s\n",
    "\n",
    "(b) The computation time with persist is less than without persist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
