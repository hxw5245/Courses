{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Spring 2022\n",
    "# Instructor: Professor John Yen\n",
    "# TA: Rupesh Prajapati \n",
    "# LAs: Lily Jakielaszek and Cayla Shan Pun\n",
    "# Lab 6: Big Data Movie Recommendations Using Spark-submit, Persist and Looping\n",
    "\n",
    "## The goals of this lab, building on Lab 6, are for you to be able to\n",
    "### - Define schema for reading a big movie rating file\n",
    "### - Understand a potential flaw for random sampling from Big Data.\n",
    "### - Sample from the big rating data in a more suitable way.\n",
    "### - Design different options for persist RDD/DF involved in the iterative cycle for hyper parameter tuning.\n",
    "### - Evaluate the run-time performance of different options of persist using the Big review data.\n",
    "### - Manage jobs in ICDS Roar \n",
    "\n",
    "## Exercises: \n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 5 points (schema definition)\n",
    "- Exercise 3: 10 points (total number of users and movies)\n",
    "- Exercise 4: 15 points (Evaluation of ALS model from random sampling)\n",
    "- Exercise 6: 10 points (Evaluation of ALS model from systematic sampling)\n",
    "- Exercise 6: 10 points (Changes to Lab7B for spark-submit to cluster)\n",
    "- Exercise 7: 15 points (Word file: Rationale of the choice of three persist.)\n",
    "- Exercise 8: 30 points (Word file: Performance of the three options of persist design.)\n",
    "## Total Points: 100 points\n",
    "\n",
    "# Due: midnight, February 20, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The first thing we need to do in each Jupyter Notebook running pyspark is to import pyspark first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once we import pyspark, we need to import \"SparkContext\".  Every spark program needs a SparkContext object\n",
    "### In order to use Spark SQL on DataFrames, we also need to import SparkSession from PySpark.SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType, IntegerType, FloatType\n",
    "from pyspark.sql.functions import col, column\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import Row\n",
    "from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We then create a Spark Session variable (rather than Spark Context) in order to use DataFrame. \n",
    "- Note: We temporarily use \"local\" as the parameter for master in this notebook so that we can test it in ICDS Roar.  However, we need to remove \".master(\"local\")\" before we submit it to ICDS to run in the cluster mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=SparkSession.builder.appName(\"Lab6A Recommendation Using Big Data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.sparkContext.setCheckpointDir(\"~/scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (5 points) \n",
    "## Add your name below.\n",
    "## Answer for Exercise 1:\n",
    "- Your Name: Haichen Wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (5 points) \n",
    "## Complete the schema definition and run the code cells below so that rating_schema can be used to read the big movie rating file (csv format) into a PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsbig_DF = ss.read.csv(\"/storage/home/hxw5245/Lab6/ratings-large.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 1: integer (nullable = true)\n",
      " |-- 122: integer (nullable = true)\n",
      " |-- 2.0: double (nullable = true)\n",
      " |-- 945544824: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ratingsbig_DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_schema = StructType([ StructField(\"UserID\", IntegerType(), False ), \\\n",
    "                            StructField(\"MovieID\", IntegerType(), True), \\\n",
    "                            StructField(\"Rating\", FloatType(), True ), \\\n",
    "                            StructField(\"RatingID\", IntegerType(), True ), \\\n",
    "                           ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_DF = ss.read.csv(\"/storage/home/hxw5245/Lab6/ratings-large.csv\", schema= rating_schema, header=False, inferSchema=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- MovieID: integer (nullable = true)\n",
      " |-- Rating: float (nullable = true)\n",
      " |-- RatingID: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ratings_DF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Investigating Size of the Big Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (10 points)\n",
    "- (a) What is the number of users in this big movie rating dataset? (5 points)\n",
    "- (b) What is the number of movies in this big movie rating dataset? (5 points)\n",
    "## After running the code cells below, enter your answer in the Markdown cell below for Answer to Exercise 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_DF = ratings_DF.select(\"UserID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|UserID|\n",
      "+------+\n",
      "|     1|\n",
      "|     1|\n",
      "|     1|\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# users_DF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We use `.dropDuplicates()` to remove duplicate rows of a PySpark DataFrame. The following code calculate the number of unique users in the large ratings dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnqUsr_DF = users_DF.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "UserCnt= UnqUsr_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_DF = ratings_DF.select(\"MovieID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnqMovie_DF = movies_DF.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MovieCnt = UnqMovie_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Count = 259137 Movie Count = 39443\n"
     ]
    }
   ],
   "source": [
    "# print(\"User Count =\", UserCnt, \"Movie Count =\", MovieCnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to Exercise 3\n",
    "- (a) The number of users is 259137.\n",
    "- (b) The number of movies is 39443."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2. Random Sampling from Big Data\n",
    "## We noticed the size of the users in the large dataset is about 400 times larger than that of the small dataset. We also noticed that the size of the movies in the large dataset is about 4 times larger than that of the small dataset. What problem will we run into if we randomly sample a small portion (e.g., 0.3%) from the large dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (15%) \n",
    "## Complete the code cell below to use a random sampled (0.3%) big movie review data to train an ALS model, then evaluate its RMS training error and RMS validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2_DF = ratings_DF.sample(withReplacement=False, fraction=0.003, seed=19).select(\"UserID\",\"MovieID\",\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2_RDD= ratings2_DF.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split ratings2_DF into training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = ratings2_RDD.randomSplit([3, 1, 1], 521)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input (UserID, MovieID) for validation and for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_RDD = training_RDD.map(lambda x: (x[0], x[1]) )\n",
    "validation_input_RDD = validation_RDD.map(lambda x: (x[0], x[1]) ) \n",
    "testing_input_RDD = test_RDD.map(lambda x: (x[0], x[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ALS.train(training_RDD, 4, seed=41, iterations=30, lambda_=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prediction_RDD = model.predictAll(training_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((185012, 70286), 4.8765629117174),\n",
       " ((185544, 1654), 2.901551927157385),\n",
       " ((18624, 910), 4.237794369522611)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_prediction_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_evaluation_RDD = training_RDD.map(lambda y: ( (y[0], y[1]), y[2]) ).join(training_prediction_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((97, 7), (5.0, 4.876108943626904)),\n",
       " ((114, 1971), (2.0, 1.9377822692076734)),\n",
       " ((170, 1200), (5.0, 4.768294074229932))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_evaluation_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = math.sqrt(training_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12554731880797582\n"
     ]
    }
   ],
   "source": [
    "# print(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prediction_RDD = model.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((18624, 4993), -2.7723966315171893),\n",
       " ((18624, 1375), -0.7301429050535124),\n",
       " ((183162, 4993), 1.5842129069877195),\n",
       " ((40888, 780), -2.228305440307202),\n",
       " ((101022, 3147), 0.7055109374368431)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation_prediction_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_evaluation_RDD = validation_RDD.map(lambda y: ( (y[0], y[1]), y[2] )).join(validation_prediction_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((121, 1270), (4.0, 1.4984482106482158)),\n",
       " ((281, 2232), (4.0, -3.384190315921497)),\n",
       " ((390, 2700), (2.5, -1.5414470004741294)),\n",
       " ((816, 70293), (3.5, -1.569558637508499)),\n",
       " ((1132, 4890), (2.0, 2.3450679053943677))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation_evaluation_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.888641479565653\n"
     ]
    }
   ],
   "source": [
    "# print(validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding 1: \n",
    "The validation error is much bigger than the training error.  The validation error is also much larger than the validation error of Lab 5. What are possible reasons to explain this?\n",
    "- The space of users and movies is much larger than that of the Lab 5.\n",
    "- Because the much larger space of users and movies, the sampled ratings is a much sparser than the rating matrix of Lab 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do next?\n",
    "While we are primarily interested in creating a recommendation model using the entire big dataset (not just using a small sample), it is desirable to check that we can generate a reasonoable recommendation model (based on ALS) using a sampled dataset before we do this in the big dataset. Therefore, the next section discusses a sampling approach from the big dataset to reduce the sparsity of the rating matrix by (1) sampling a small group of users, and (2) using all reviews from this samll group of sampled users to create training, testing, and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3. Sampling to Reduce the Sparsity of User and Movie Ratings Matrix\n",
    "## To reduce the sparsity of the User X Movies rating matrix, we can sample a small subset of Users (e.g., 0.3 %) first, then join it with the big rating dataset to gather all reviews by these sampled users, whihc is used for creating training, testing, and validation data.\n",
    "## Benefits of this sampling approach: \n",
    "- The sparsity of the rating matrix is significantly reduced because the number of users is much smaller than that of the previous sampled dataset.\n",
    "- Such a small sampled rating matrix can be used to evaluate the feasibility of the ALS-based recommendation model for the dataset before we construct the model using the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_ratio = 0.003\n",
    "sampled_User_DF = UnqUsr_DF.sample(withReplacement=False, fraction=sampling_ratio, seed=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_rating_DF = sampled_User_DF.join(ratings_DF, \"UserID\", \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "763"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_User_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70023"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_rating_DF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2_DF = sampled_rating_DF.select(\"UserID\",\"MovieID\",\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings2_RDD = ratings2_DF.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.2 Split Systematic Sampled Data into Training Data, Validation Data, and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = ratings2_RDD.randomSplit([3, 1, 1], 137)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split sampled data into training and validating data for constructing and evaluating an ALS model using a specific rank, iteration, and regularization parameter.  The purpose of this step is to check whether the performance of the sampled data from Big Data is comparable to that constructed from the small data (in Lab 6). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_RDD = training_RDD.map(lambda x: (x[0], x[1]) )\n",
    "validation_input_RDD = validation_RDD.map(lambda x: (x[0], x[1]) ) \n",
    "testing_input_RDD = test_RDD.map(lambda x: (x[0], x[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ALS.train(training_RDD, 4, seed=37, iterations=30, lambda_=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3.3 Using the constructed model to evaluate RMS of training data and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (10%)\n",
    "## Complete the code cells below for training and evaluating the recommendation model constructed from systematically sampled big review data.\n",
    "## Note: Running some of the cells may take a while, due to the size of the data.  Make sure you do NOT proceed to run another cell while a cell is still running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prediction_RDD = model2.predictAll(training_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((79112, 1084), 3.6004677829181446),\n",
       " ((216620, 1084), 4.155774459738865),\n",
       " ((104198, 1084), 4.016597102984555)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_prediction_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_evaluation_RDD = training_RDD.map(lambda y: ( (y[0], y[1]), y[2]) ).join(training_prediction_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((671, 1013), (4.5, 4.380425656442638)),\n",
       " ((671, 54286), (5.0, 4.427193653959)),\n",
       " ((895, 7361), (4.0, 4.070981248196606))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_evaluation_RDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_error = math.sqrt(training_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5776585296698834\n"
     ]
    }
   ],
   "source": [
    "# print(training_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prediction_RDD = model2.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((198460, 1084), 3.986631426707632),\n",
       " ((229348, 1084), 3.4505677299137787),\n",
       " ((145016, 1084), 2.632263508102838),\n",
       " ((76115, 1084), 3.874546301189566),\n",
       " ((186781, 1084), 3.7997169964520863)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation_prediction_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_evaluation_RDD = validation_RDD.map(lambda y: ( (y[0], y[1]), y[2]) ).join(validation_prediction_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((895, 4027), (4.5, 4.037829232108405)),\n",
       " ((895, 112552), (5.0, 4.661475249392744)),\n",
       " ((1068, 1372), (3.0, 3.2516056284110277)),\n",
       " ((1068, 2916), (4.0, 3.4041800631735946)),\n",
       " ((1068, 3481), (3.0, 3.8319677862822097))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation_evaluation_RDD.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9561062191200534\n"
     ]
    }
   ],
   "source": [
    "# print(validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding 2: \n",
    "The validation error of the second model, constructed using all reviews from a subset of users, is closer to the testing error than what we observed in Finding 1 (for the first model, constructed using a random sample from the big dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Hyperparameter Tuning\n",
    "\n",
    "## Like Lab 5, you will need to iterate through all possible combination of a set of values for three hyperparameters for ALS Recommendation Model:\n",
    "- rank (k)\n",
    "- regularization\n",
    "- iterations \n",
    "## Each hyperparameter value combination is used to construct an ALS recommendation model using training data, but evaluate using Evaluation Data\n",
    "## The evaluation results are saved in a Pandas DataFrame \n",
    "``\n",
    "hyperparams_eval_df\n",
    "``\n",
    "## The best hyperprameter value combination is stored in 4 variables\n",
    "``\n",
    "best_k, best_regularization, best_iterations, and lowest_validation_error\n",
    "``\n",
    "# However, you do NOT NEED TO execute the code below in Local mode, since you have tested the construction and evaluation of ALS model using a sampled data from the big review data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (10 points) \n",
    "## Duplicate this Jupyter Notebook (Lab6), name the new copy \"Lab6A.ipynb\". \n",
    "- Make all modifications to Lab6A so that it is ready to be submitted to the cluster (but don't add any persist yet). \n",
    "- In addition, comment out codes in Section 7.1, 7.2, and 7.3. Save the modified Lab6A notebook, and export it into Lab6A.py. \n",
    "- Run spark-submit of Lab6A.py in ICDS cluster (following instructions of Lab 3) to generate the results of hyper-parameter tuning for the big dataset, as well as the time it takes (shown in the log file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings4_DF = ratings_DF.select(\"UserID\",\"MovieID\",\"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings4_RDD = ratings4_DF.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_RDD, validation_RDD, test_RDD = ratings4_RDD.randomSplit([3, 1, 1], 137)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_RDD = training_RDD.map(lambda x: (x[0], x[1]) )\n",
    "validation_input_RDD = validation_RDD.map(lambda x: (x[0], x[1]) ) \n",
    "testing_input_RDD = test_RDD.map(lambda x: (x[0], x[1]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize a Pandas DataFrame to store evaluation results of all combination of hyper-parameter settings\n",
    "hyperparams_eval_df = pd.DataFrame( columns = ['k', 'regularization', 'iterations', 'validation RMS', 'testing RMS'] )\n",
    "# initialize index to the hyperparam_eval_df to 0\n",
    "index =0 \n",
    "# initialize lowest_error\n",
    "lowest_validation_error = float('inf')\n",
    "# Set up the possible hyperparameter values to be evaluated\n",
    "iterations_list = [15, 30]\n",
    "regularization_list = [0.1, 0.2]\n",
    "rank_list = [4, 8, 12]\n",
    "for k in rank_list:\n",
    "    for regularization in regularization_list:\n",
    "        for iterations in iterations_list:\n",
    "            seed = 37\n",
    "            # Construct a recommendation model using a set of hyper-parameter values and training data\n",
    "            model = ALS.train(training_RDD, k, seed=seed, iterations=iterations, lambda_=regularization)\n",
    "            # Evaluate the model using evalution data\n",
    "            # map the output into ( (userID, movieID), rating ) so that we can join with actual evaluation data\n",
    "            # using (userID, movieID) as keys.\n",
    "            validation_prediction_RDD= model.predictAll(validation_input_RDD).map(lambda x: ( (x[0], x[1]), x[2] )   )\n",
    "            validation_evaluation_RDD = validation_RDD.map(lambda y: ( (y[0], y[1]), y[2]) ).join(validation_prediction_RDD)\n",
    "            # Calculate RMS error between the actual rating and predicted rating for (userID, movieID) pairs in validation dataset\n",
    "            validation_error = math.sqrt(validation_evaluation_RDD.map(lambda z: (z[1][0] - z[1][1])**2).mean())\n",
    "            # Save the error as a row in a pandas DataFrame\n",
    "            hyperparams_eval_df.loc[index] = [k, regularization, iterations, validation_error, float('inf')]\n",
    "            index = index + 1\n",
    "            # Check whether the current error is the lowest\n",
    "            if validation_error < lowest_validation_error:\n",
    "                best_k = k\n",
    "                best_regularization = regularization\n",
    "                best_iterations = iterations\n",
    "                best_index = index - 1\n",
    "                lowest_validation_error = validation_error\n",
    "print('The best rank k is ', best_k, ', regularization = ', best_regularization, ', iterations = ',\\\n",
    "      best_iterations, '. Validation Error =', lowest_validation_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Testing Data to Evaluate the Model built using the Best Hyperparameters                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.5 Evaluate the best hyperparameter combination using testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 37\n",
    "model = ALS.train(training_RDD, best_k, seed=seed, iterations=best_iterations, lambda_=best_regularization)\n",
    "testing_prediction_RDD=model.predictAll(testing_input_RDD).map(lambda x: ((x[0], x[1]), x[2]))\n",
    "testing_evaluation_RDD= test_RDD.map(lambda x: ((x[0], x[1]), x[2])).join(testing_prediction_RDD)\n",
    "testing_error = math.sqrt(testing_evaluation_RDD.map(lambda x: (x[1][0]-x[1][1])**2).mean())\n",
    "print('The Testing Error for rank k =', best_k, ' regularization = ', best_regularization, ', iterations = ', \\\n",
    "      best_iterations, ' is : ', testing_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(best_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the Testing RMS in the DataFrame\n",
    "hyperparams_eval_df.loc[best_index]=[best_k, best_regularization, best_iterations, lowest_validation_error, testing_error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema3= StructType([ StructField(\"k\", FloatType(), True), \\\n",
    "                      StructField(\"regularization\", FloatType(), True ), \\\n",
    "                      StructField(\"iterations\", FloatType(), True), \\\n",
    "                      StructField(\"Validation RMS\", FloatType(), True), \\\n",
    "                      StructField(\"Testing RMS\", FloatType(), True) \\\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the pandas DataFrame that stores validation errors of all hyperparameters and the testing error for the best model to Spark DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParams_Tuning_DF = ss.createDataFrame(hyperparams_eval_df, schema3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify the output path so that your output results can be saved in a directory.\n",
    "# Notice: Remember to change the output_path directory before each spark-submit in cluster. Otherwise, the spark_submit will NOT run the action (saveAsTextFile) successfully due to being unable to write into an existing directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = \"/storage/home/hxw5245/Lab6/Lab6_A\"\n",
    "HyperParams_Tuning_DF.rdd.saveAsTextFile(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7 (15%)\n",
    "## Describe the rationale of three persists you added to the code in a word file (to be submitted for Lab 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8 (30%)\n",
    "## In the same word file, include two information for four versions of running Lab6B in the cluster: (a) the best hyperparameters, the validation error and the testing error, and (b) the run time (real, user, and sys) in ICDS-Roar using the configuration in the instruction for running spark-submit in ICDS-Roar.\n",
    "- 1. The baseline (no persist) Lab6A\n",
    "- 2. With three persists on RDDs: Lab6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
