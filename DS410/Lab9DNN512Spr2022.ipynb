{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS/CMPSC 410 Spring 2021\n",
    "# Instructor: Professor John Yen\n",
    "# TA: Rupesh Prajapati \n",
    "# LAs: Lily Jakielaszek and Cayla Shan Pun\n",
    "# Lab 9: Deep Neural Networks\n",
    "\n",
    "## The goals of this lab are for you to be able to\n",
    "### - Use tensorflow and keras to implement a deep learning application (MNIST)\n",
    "### - Be able to assess the result of DNN learning using validation data\n",
    "### - Be able to identify potential overfitting risk of a DNN model\n",
    "### - Be able to reduce potential overfitting risk by adjusting epoch and size of batch\n",
    "### - Be able to compare learning outcomes of different DNN architectures\n",
    "\n",
    "## Exercises: \n",
    "- Exercise 1: 5 points\n",
    "- Exercise 2: 10 points\n",
    "- Exercise 3: 10 points\n",
    "- Exercise 4: 15 points\n",
    "- Exercise 5: 20 points\n",
    "- Exercise 6: 25 points\n",
    "\n",
    "## Total Points (Lab): 85 points\n",
    "\n",
    "# Due: midnight, April 21 (Thursday), 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install tensorflow and keras\n",
    "The first thing to do is to install tensorflow and keras in your ICDS Roar environment.\n",
    "- Open a terminal window in Jupyter Lab\n",
    "- Type the following in ther terminal window \n",
    "```pip install tensorflow```\n",
    "- After the installation of tensorflow completes, type the following in the terminal window \n",
    "```pip install keras```\n",
    "- Wait until the installation completes. Then run the \"import tensorflow as tf\" in Jupyter Notebook and continue based on the instructions on Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 (5 points)\n",
    "Enter your name here: Haichen Wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A Pre-processing Input Data\n",
    "In this lab, we pre-process the 28 by 28 input image into a vector of 784 (28*28) input features (one for each pixel).  An alternative is to use Convolutionary Deep Neural Networks\n",
    "directly on 28 by 28 input images without reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images2 = train_images.reshape(60000, 784)\n",
    "test_images2 = test_images.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
       "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
       "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
       "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
       "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
       "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
       "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
       "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
       "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
       "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
       "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
       "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
       "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
       "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
       "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
       "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
       "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
       "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
       "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
       "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
       "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
       "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
       "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
       "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
       "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The computation of nodes in a DNN is based on floating point. Therefore, we need to convert the input data (reshpaed image data) to floating point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images3 = train_images2.astype('float32')\n",
    "test_images3 = test_images2.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  18.,\n",
       "        18.,  18., 126., 136., 175.,  26., 166., 255., 247., 127.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        30.,  36.,  94., 154., 170., 253., 253., 253., 253., 253., 225.,\n",
       "       172., 253., 242., 195.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  49., 238., 253., 253., 253., 253.,\n",
       "       253., 253., 253., 253., 251.,  93.,  82.,  82.,  56.,  39.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "        18., 219., 253., 253., 253., 253., 253., 198., 182., 247., 241.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  80., 156., 107., 253.,\n",
       "       253., 205.,  11.,   0.,  43., 154.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  14.,   1., 154., 253.,  90.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "       139., 253., 190.,   2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  11., 190., 253.,  70.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  35., 241., 225., 160., 108.,   1.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  81., 240.,\n",
       "       253., 253., 119.,  25.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  45., 186., 253., 253., 150.,  27.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  16.,  93., 252., 253., 187.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 249., 253.,\n",
       "       249.,  64.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  46., 130., 183., 253., 253., 207.,   2.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,  39., 148., 229., 253., 253., 253.,\n",
       "       250., 182.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  24., 114.,\n",
       "       221., 253., 253., 253., 253., 201.,  78.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  23.,  66., 213., 253., 253., 253., 253., 198.,  81.,\n",
       "         2.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  18., 171., 219., 253., 253.,\n",
       "       253., 253., 195.,  80.,   9.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  55.,\n",
       "       172., 226., 253., 253., 253., 253., 244., 133.,  11.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0., 136., 253., 253., 253., 212., 135.,\n",
       "       132.,  16.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The value of each pixel ranges from 0 (white) to 255 (darkest black). We want to transform the input into the range of [0, 1]. This is easily done by dividing the original pixel value by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images3 /= 255\n",
    "test_images3 /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 (10 points)\n",
    "Fill in the shape of input data. Specify the number of nodes in each hidden layer (3 hidden layers total) for the DNN architecture.\n",
    "Recommended number of nodes in each hidden layers: 512. The last layer is the output layer. Hence, it should have\n",
    "10 nodes (one for each signal digit character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential( [ \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'), \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'),\n",
    "                             tf.keras.layers.Dense(10, activation='softmax'), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 (10 points)\n",
    "Fill in the training input and testing input (validation data) in the code below, both\n",
    "of which should be images after the three transformation steps (reshape, astype, and scaling).\n",
    "In this first DNN learning, we use a batch size of 128.  The default batch size is 32. We set vertose to 1 so that we can see the result of evaluating both training data and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "469/469 [==============================] - 7s 13ms/step - loss: 0.2145 - accuracy: 0.9341 - val_loss: 0.1043 - val_accuracy: 0.9676\n",
      "Epoch 2/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0815 - accuracy: 0.9745 - val_loss: 0.0842 - val_accuracy: 0.9741\n",
      "Epoch 3/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0543 - accuracy: 0.9831 - val_loss: 0.0749 - val_accuracy: 0.9766\n",
      "Epoch 4/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: 0.0712 - val_accuracy: 0.9792\n",
      "Epoch 5/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0326 - accuracy: 0.9896 - val_loss: 0.0723 - val_accuracy: 0.9795\n",
      "Epoch 6/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 0.0749 - val_accuracy: 0.9796\n",
      "Epoch 7/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0264 - accuracy: 0.9913 - val_loss: 0.0841 - val_accuracy: 0.9791\n",
      "Epoch 8/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0235 - accuracy: 0.9922 - val_loss: 0.0751 - val_accuracy: 0.9805\n",
      "Epoch 9/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0169 - accuracy: 0.9945 - val_loss: 0.0865 - val_accuracy: 0.9788\n",
      "Epoch 10/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0170 - accuracy: 0.9946 - val_loss: 0.0888 - val_accuracy: 0.9790\n",
      "Epoch 11/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0128 - accuracy: 0.9958 - val_loss: 0.0956 - val_accuracy: 0.9800\n",
      "Epoch 12/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0165 - accuracy: 0.9948 - val_loss: 0.0988 - val_accuracy: 0.9786\n",
      "Epoch 13/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0143 - accuracy: 0.9958 - val_loss: 0.0925 - val_accuracy: 0.9795\n",
      "Epoch 14/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0156 - accuracy: 0.9954 - val_loss: 0.0756 - val_accuracy: 0.9838\n",
      "Epoch 15/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0998 - val_accuracy: 0.9805\n",
      "Epoch 16/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0118 - accuracy: 0.9965 - val_loss: 0.0914 - val_accuracy: 0.9818\n",
      "Epoch 17/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.0984 - val_accuracy: 0.9816\n",
      "Epoch 18/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0116 - accuracy: 0.9963 - val_loss: 0.0919 - val_accuracy: 0.9815\n",
      "Epoch 19/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.1018 - val_accuracy: 0.9803\n",
      "Epoch 20/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0105 - accuracy: 0.9969 - val_loss: 0.0971 - val_accuracy: 0.9806\n",
      "Epoch 21/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0076 - accuracy: 0.9979 - val_loss: 0.0881 - val_accuracy: 0.9826\n",
      "Epoch 22/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.1081 - val_accuracy: 0.9804\n",
      "Epoch 23/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.1060 - val_accuracy: 0.9815\n",
      "Epoch 24/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0980 - val_accuracy: 0.9814\n",
      "Epoch 25/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.0851 - val_accuracy: 0.9843\n",
      "Epoch 26/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0073 - accuracy: 0.9977 - val_loss: 0.0996 - val_accuracy: 0.9823\n",
      "Epoch 27/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.1041 - val_accuracy: 0.9833\n",
      "Epoch 28/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 0.1051 - val_accuracy: 0.9818\n",
      "Epoch 29/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 0.1012 - val_accuracy: 0.9829\n",
      "Epoch 30/30\n",
      "469/469 [==============================] - 6s 12ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.1312 - val_accuracy: 0.9758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b6bd5b77eb0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images3, train_labels, batch_size=128, epochs=30, verbose=1, \\\n",
    "          validation_data=(test_images3, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 (15 points)\n",
    "- (a) Does the final DNN model learned indicate a risk for overfitting? Explain your answer. (5 points)\n",
    "- (b) Which epoch, you believe, generated the best model? Explain the rationale of your decision. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers to Exercise 4\n",
    "- (a) Yes, the loss measure for validation data is much worse than the loss measure for training data.\n",
    "- (b) Epoch 14 is the best, because it has the highest validation accuracy, 0.9838, and its validation loss, 0.0756, is relatively smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 (20 points)\n",
    "Change the batch size to 1000, complete the following code (keeping the number of nodes in each layer identical to Exercise 2. Compare results of the learned DNN to that of the previous one. Answer the following questions in the Markdown cell at the bottom of the Notebook.\n",
    "- (a) Which epoch generates the best DNN model? Why? (10 points)\n",
    "- (b) Based on the results of Exercise 4 and 5, which choice of batch size is better? Why? (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential( [ \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'), \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'),\n",
    "                             tf.keras.layers.Dense(10, activation='softmax'), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60/60 [==============================] - 4s 61ms/step - loss: 0.4325 - accuracy: 0.8798 - val_loss: 0.1675 - val_accuracy: 0.9507\n",
      "Epoch 2/30\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 0.1290 - accuracy: 0.9616 - val_loss: 0.1079 - val_accuracy: 0.9666\n",
      "Epoch 3/30\n",
      "60/60 [==============================] - 4s 58ms/step - loss: 0.0816 - accuracy: 0.9748 - val_loss: 0.0810 - val_accuracy: 0.9752\n",
      "Epoch 4/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0566 - accuracy: 0.9827 - val_loss: 0.0805 - val_accuracy: 0.9738\n",
      "Epoch 5/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0388 - accuracy: 0.9879 - val_loss: 0.0687 - val_accuracy: 0.9788\n",
      "Epoch 6/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0275 - accuracy: 0.9916 - val_loss: 0.0702 - val_accuracy: 0.9804\n",
      "Epoch 7/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0185 - accuracy: 0.9945 - val_loss: 0.0630 - val_accuracy: 0.9819\n",
      "Epoch 8/30\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0756 - val_accuracy: 0.9787\n",
      "Epoch 9/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0103 - accuracy: 0.9971 - val_loss: 0.0748 - val_accuracy: 0.9798\n",
      "Epoch 10/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0781 - val_accuracy: 0.9786\n",
      "Epoch 11/30\n",
      "60/60 [==============================] - 4s 58ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 0.0750 - val_accuracy: 0.9799\n",
      "Epoch 12/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0057 - accuracy: 0.9983 - val_loss: 0.0853 - val_accuracy: 0.9786\n",
      "Epoch 13/30\n",
      "60/60 [==============================] - 4s 60ms/step - loss: 0.0063 - accuracy: 0.9983 - val_loss: 0.0812 - val_accuracy: 0.9807\n",
      "Epoch 14/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0901 - val_accuracy: 0.9789\n",
      "Epoch 15/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0119 - accuracy: 0.9960 - val_loss: 0.0997 - val_accuracy: 0.9755\n",
      "Epoch 16/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0071 - accuracy: 0.9977 - val_loss: 0.0833 - val_accuracy: 0.9816\n",
      "Epoch 17/30\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0906 - val_accuracy: 0.9800\n",
      "Epoch 18/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.0798 - val_accuracy: 0.9829\n",
      "Epoch 19/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0853 - val_accuracy: 0.9819\n",
      "Epoch 20/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0838 - val_accuracy: 0.9829\n",
      "Epoch 21/30\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.0016 - accuracy: 0.9996 - val_loss: 0.0850 - val_accuracy: 0.9834\n",
      "Epoch 22/30\n",
      "60/60 [==============================] - 4s 58ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.1035 - val_accuracy: 0.9783\n",
      "Epoch 23/30\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0945 - val_accuracy: 0.9791\n",
      "Epoch 24/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0121 - accuracy: 0.9959 - val_loss: 0.0908 - val_accuracy: 0.9789\n",
      "Epoch 25/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.0803 - val_accuracy: 0.9814\n",
      "Epoch 26/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0039 - accuracy: 0.9986 - val_loss: 0.0959 - val_accuracy: 0.9807\n",
      "Epoch 27/30\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.0057 - accuracy: 0.9980 - val_loss: 0.1024 - val_accuracy: 0.9800\n",
      "Epoch 28/30\n",
      "60/60 [==============================] - 4s 59ms/step - loss: 0.0062 - accuracy: 0.9980 - val_loss: 0.0995 - val_accuracy: 0.9803\n",
      "Epoch 29/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0875 - val_accuracy: 0.9828\n",
      "Epoch 30/30\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0981 - val_accuracy: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b6bd6795fa0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_images3, train_labels, batch_size=1000, epochs=30, verbose=1, \\\n",
    "          validation_data=(test_images3, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers to Exercise 5 (20 points)\n",
    "- (a) (10 points)\n",
    "\n",
    "Epoch 7 is the best, because it has the smallest validation , 0.0630, and its validation accuracy, 0.9819, is larger relatively.\n",
    "\n",
    "- (b) (10 points)\n",
    "\n",
    "1000 batch size is better. A larger batch size provides more training data to allow the weight adjustments to consider all of them at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6 (25 points)\n",
    "Copy code above for training a DNN of only two layers, using the same number of nodes in each layer as you chose for Exercise 2 and 4, but use the batch size that gave the better result.\n",
    "- (a) What is the performance result of the DNN learned? (10 points)\n",
    "- (a) Will you choose this DNN over the one with three layers?  Why? (10 points)\n",
    "- (b) Compare the overfitting risk of this DNN with that of the previous two? (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer to Exercise 6 \n",
    "- (a) It has high validation accuracy and the validation loss is smaller than previous two in general. \n",
    "- (b) No, this DNN performs worse than the other two because of the overfitting. The loss measure for validation data is much worse than the loss measure for training data, and it decreases as training proceeds.\n",
    "- (c) It has higher overfitting risk than previous two models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = tf.keras.Sequential( [ \\\n",
    "                             tf.keras.layers.Dense(512, activation='relu', input_shape=(784,)),\\\n",
    "                             tf.keras.layers.Dense(512, activation='relu'),\n",
    "                             tf.keras.layers.Dense(10, activation='softmax'), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60/60 [==============================] - 3s 42ms/step - loss: 0.4559 - accuracy: 0.8740 - val_loss: 0.1875 - val_accuracy: 0.9473\n",
      "Epoch 2/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.1553 - accuracy: 0.9544 - val_loss: 0.1221 - val_accuracy: 0.9648\n",
      "Epoch 3/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0984 - accuracy: 0.9718 - val_loss: 0.0920 - val_accuracy: 0.9716\n",
      "Epoch 4/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0707 - accuracy: 0.9800 - val_loss: 0.0833 - val_accuracy: 0.9738\n",
      "Epoch 5/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0543 - accuracy: 0.9841 - val_loss: 0.0708 - val_accuracy: 0.9791\n",
      "Epoch 6/30\n",
      "60/60 [==============================] - 2s 41ms/step - loss: 0.0417 - accuracy: 0.9877 - val_loss: 0.0698 - val_accuracy: 0.9789\n",
      "Epoch 7/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0329 - accuracy: 0.9910 - val_loss: 0.0709 - val_accuracy: 0.9786\n",
      "Epoch 8/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0242 - accuracy: 0.9933 - val_loss: 0.0607 - val_accuracy: 0.9806\n",
      "Epoch 9/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0175 - accuracy: 0.9957 - val_loss: 0.0664 - val_accuracy: 0.9805\n",
      "Epoch 10/30\n",
      "60/60 [==============================] - 2s 41ms/step - loss: 0.0133 - accuracy: 0.9972 - val_loss: 0.0598 - val_accuracy: 0.9828\n",
      "Epoch 11/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0098 - accuracy: 0.9981 - val_loss: 0.0608 - val_accuracy: 0.9816\n",
      "Epoch 12/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0079 - accuracy: 0.9987 - val_loss: 0.0626 - val_accuracy: 0.9815\n",
      "Epoch 13/30\n",
      "60/60 [==============================] - 2s 41ms/step - loss: 0.0057 - accuracy: 0.9992 - val_loss: 0.0654 - val_accuracy: 0.9816\n",
      "Epoch 14/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0046 - accuracy: 0.9995 - val_loss: 0.0692 - val_accuracy: 0.9815\n",
      "Epoch 15/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0039 - accuracy: 0.9996 - val_loss: 0.0638 - val_accuracy: 0.9818\n",
      "Epoch 16/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.0647 - val_accuracy: 0.9825\n",
      "Epoch 17/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.0661 - val_accuracy: 0.9824\n",
      "Epoch 18/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0660 - val_accuracy: 0.9829\n",
      "Epoch 19/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9829\n",
      "Epoch 20/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 9.8984e-04 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 0.9831\n",
      "Epoch 21/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 7.9278e-04 - accuracy: 1.0000 - val_loss: 0.0673 - val_accuracy: 0.9834\n",
      "Epoch 22/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 7.0870e-04 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9830\n",
      "Epoch 23/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 6.2163e-04 - accuracy: 1.0000 - val_loss: 0.0690 - val_accuracy: 0.9827\n",
      "Epoch 24/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 5.3689e-04 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9828\n",
      "Epoch 25/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 4.7458e-04 - accuracy: 1.0000 - val_loss: 0.0700 - val_accuracy: 0.9833\n",
      "Epoch 26/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 4.3019e-04 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 0.9830\n",
      "Epoch 27/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 3.9196e-04 - accuracy: 1.0000 - val_loss: 0.0715 - val_accuracy: 0.9829\n",
      "Epoch 28/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 3.5997e-04 - accuracy: 1.0000 - val_loss: 0.0722 - val_accuracy: 0.9831\n",
      "Epoch 29/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 3.3018e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9833\n",
      "Epoch 30/30\n",
      "60/60 [==============================] - 2s 40ms/step - loss: 2.9904e-04 - accuracy: 1.0000 - val_loss: 0.0726 - val_accuracy: 0.9829\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b6c0786ab80>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_images3, train_labels, batch_size=1000, epochs=30, verbose=1, \\\n",
    "          validation_data=(test_images3, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
