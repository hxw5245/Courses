{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","name":"python","file_extension":".py","version":"3.5.4","pygments_lexer":"ipython3","codemirror_mode":{"version":3,"name":"ipython"}},"colab":{"name":"Lab5WeiHaichen.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"collapsed":true,"trusted":false,"id":"JjYs9GEPIb3o"},"source":["# Penn State DS 200 Fall 2020\n","# Lab 5 Tweets Gathering \n","\n","## Instructor: Professor John Yen\n","## TA: Rupesh Prajapati\n","## LA: Nathan Tack\n","\n","# Learning Objectives:\n","- Be able to apply and obtain approval for a Twitter Developer Account.\n","- Be able to obtain API keys and Access Tokens, which are needed for gathering tweets from Twitter.\n","- Be able to identify a set of keywords and hashtags for sampling tweets relevant to a topic of interest.\n","- Be able to install Tweepy, a Python library/module for tathering tweets using Twitter API.\n","- Be able to use API keys and Access Tokens to run a Tweets gathering Python code.\n","- Be able to read Tweets gathered as a Table.\n","\n","# Exercises: 4\n","- Exercise 1: 20 points\n","- Exercise 2: 10 points\n","- Exercise 3: 10 points\n","- Exercise 4: 5 points\n","- Exercise 5: 10 points\n","\n","# Total Points: 55 points\n","\n","# Due Date: 5 pm, September 28th, 2020\n","\n","### Install Tweepy\n","The first thing we will do is to install a tweepy, a Python library/module for gathering tweets using Twitter API."]},{"cell_type":"code","metadata":{"trusted":true,"id":"muQMM1a-Ib3r","executionInfo":{"status":"ok","timestamp":1601264128541,"user_tz":240,"elapsed":3852,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"7bb86ea4-e1af-428e-b622-db22decd9d48","colab":{"base_uri":"https://localhost:8080/","height":207}},"source":["!pip install tweepy"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tweepy in /usr/local/lib/python3.6/dist-packages (3.6.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.3.0)\n","Requirement already satisfied: PySocks>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.7.1)\n","Requirement already satisfied: requests>=2.11.1 in /usr/local/lib/python3.6/dist-packages (from tweepy) (2.23.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tweepy) (1.15.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.11.1->tweepy) (3.0.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"4UQuP_xxIb4A","executionInfo":{"status":"ok","timestamp":1601264131511,"user_tz":240,"elapsed":611,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}}},"source":["import tweepy\n","from tweepy import OAuthHandler\n","from tweepy import Stream\n","from tweepy.streaming import StreamListener\n","\n","import sys\n","import os\n","import json\n","import time\n","import datetime\n","import re\n","\n","import pandas as pd"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KOCnp0wTjHxC"},"source":["# Mounting Google Drive\n","\n","Like the previous labs, we need to mount Google Drive so that the collected tweets can be saved there."]},{"cell_type":"code","metadata":{"id":"UGSva-CQjY9R","executionInfo":{"status":"ok","timestamp":1601264154575,"user_tz":240,"elapsed":20859,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"1bb7fc41-a193-4de9-950f-559e66a7522b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AROYRsJ5Ib4P"},"source":["### Python Code for Gathering Tweets\n","The following code defines a group of code that, together, \"listens\" (responds) to tweets (sent from Twitter API) that match the keywords and hashtags specified.  The code also filters out non-English tweets, and performs some simple preprocessing (e.g., remove non-ASCII characters in the body of the tweet), so that we do not need to worry about them later."]},{"cell_type":"code","metadata":{"trusted":true,"id":"djq6inEQIb4R","executionInfo":{"status":"ok","timestamp":1601264173068,"user_tz":240,"elapsed":821,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}}},"source":["class MyListener(StreamListener):\n","    def __init__(self, raw_file, csv_file, text_file, max_num=300):\n","        super().__init__()\n","        self.raw_file = raw_file\n","        self.csv_file = csv_file\n","        self.text_file = text_file\n","        self.max_num = max_num\n","        self.count = 0\n","        self.start_time = time.time()\n","\n","    def on_data(self, data):\n","        # Filter out special cases\n","        if data.startswith('{\"limit\":'):\n","            return\n","\n","        # Filter out non-English tweets\n","        tweet = json.loads(data)\n","        if tweet['lang'] != 'en':\n","            return\n","        # if 'retweeted_status' in tweet:\n","        #     return\n","\n","        # Extract fields from tweet and write to csv_file\n","        user_id = tweet['user']['id']\n","        user_name = tweet['user']['name']\n","        tweet_time = tweet['created_at']\n","        location = tweet['user']['location']\n","        text = tweet['text'].strip().replace('\\n', ' ').replace('\\t', ' ')\n","\n","        # Remove non-ASCII characters and commas in user_name and location\n","        if user_name is not None:\n","            user_name = ''.join([c if ord(c) < 128 else '' for c in user_name])\n","            user_name = user_name.replace(',', '')\n","        if location is not None:\n","            location = ''.join([c if ord(c) < 128 else '' for c in location])\n","            location = location.replace(',', '')\n","\n","        # Remove non-ASCII characters in text\n","        text = ''.join([c if ord(c) < 128 else '' for c in text])\n","        # Replace commas with space\n","        text = text.replace(',', ' ')\n","        # Replace double quotes with blanks\n","        text = re.sub(r'\\\"', '', text)\n","        # Replace consecutive underscores with space\n","        text = re.sub(r'[_]{2,}', ' ', text)\n","        # Remove all consecutive whitespace characters\n","        text = ' '.join(text.split())\n","\n","        # Check if csv_file, text_file exist\n","        # If not, create them and write the heads\n","        if not os.path.isfile(self.csv_file):\n","            with open(self.csv_file, 'w') as f:\n","                f.write(','.join(['user_id', 'user_name', 'tweet_time', 'location', 'text']) + '\\n')\n","        if not os.path.isfile(self.text_file):\n","            with open(self.text_file, 'w') as f:\n","                f.write('text\\n')\n","\n","        with open(self.raw_file, 'a') as f_raw, open(self.csv_file, 'a') as f_csv, open(self.text_file, 'a') as f_text:\n","            # Write to files\n","            f_raw.write(data.strip() + '\\n')\n","            f_csv.write(','.join(map(str, [user_id, user_name, tweet_time, location, text])) + '\\n')\n","            f_text.write(text + '\\n')\n","\n","            # Increment count\n","            self.count += 1\n","            # if self.count % 10 == 0 and self.count > 0:\n","            sys.stdout.write('\\r{}/{} tweets downloaded'.format(self.count, self.max_num))\n","            sys.stdout.flush()\n","\n","            # Check if reaches the maximum tweets number limit\n","            if self.count == self.max_num:\n","                print('\\nMaximum number reached.')\n","                end_time = time.time()\n","                elapse = end_time - self.start_time\n","                print('It took {} seconds to download {} tweets'.format(elapse, self.max_num))\n","                sys.exit(0)\n","\n","    def on_error(self, status):\n","        print(status)\n","        return True\n","\n","# Get the str representation of the current date and time    \n","def current_datetime_str():\n","    return format(datetime.datetime.now(), \"%Y-%m-%d_%H-%M-%S\")"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0PX0u0CAJUTW"},"source":["# Establish a Twitter Developer Account\n","\n","Follow the instructions in the slides for Lab5 to obtain the approval for your Twitter Developer Account.\n"]},{"cell_type":"markdown","metadata":{"id":"79T0v8xnIb4b"},"source":["## Exercise 1 (20 points)\n","Paste your API Key, API Secret Key, Access Token, and Access Token Secret into the four strings below, which were assigned to 4 corresponding variables: consumer_key, consumer_secret, access_token, and access_secret for obtaining authentication from Twitter API before \n","real-time Tweets (that match your keywords and hashtags) can be gathered by the Python code. \n","\n","#### Note: Make sure you copy each key exactly as they are.  Especially, pay attention to the first character and the last character to make sure you did not miss any of them.  Also, double check you did not accidentently include space or left parenthesis when you copy keys and token.\n","#### Create a keywords.txt file and upload it from your computer to Google Drive under a Tweets folder in DS200Labs"]},{"cell_type":"code","metadata":{"trusted":true,"id":"I_60TpOCIb4e","executionInfo":{"status":"error","timestamp":1601264975190,"user_tz":240,"elapsed":478679,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"13ec7f60-5717-40c7-ba39-ae577bd43f46","colab":{"base_uri":"https://localhost:8080/","height":626}},"source":["def main():\n","    # Path for Google Drive for reading keywords and writing Tweets Gathered\n","    data_directory ='/content/drive/My Drive/DS200Labs/Tweets/'\n","    # Paste your keys and token below.  \n","    consumer_key = 'aOqzIZZwckT6T88ryZpULoxTu'\n","    consumer_secret = 'rDIk0sSLrrMY4gLb0OjFrQ1KYZ8xKQCotEsieTj7G1UZcNHpDB'\n","    access_token = '1304306874746109952-bfa4p5ULyXwKuVJIPsOXnGhbbwrtvF'\n","    access_secret = 'G3MVXUPMCMdpbhGz0fCMGYC8NTpvxWqqBysoIK6sh3RDd'\n","\n","\n","    auth = OAuthHandler(consumer_key, consumer_secret)\n","    auth.set_access_token(access_token, access_secret)\n","    api = tweepy.API(auth)\n","\n","    # Welcome\n","    print('===========================================================')\n","    print('Welcome to the user interface of gathering tweets pipeline!')\n","    print('You can press \"Ctrl+C\" at anytime to abort the program.')\n","    print('===========================================================')\n","    print()\n","\n","    # Prompt for input keywords\n","    methods = ['manual', 'file']\n","    print('How do you want to specify your key words?')\n","    while True:\n","        m = input('Type \"manual\" or \"file\" >>> ')\n","        if m in methods:\n","            break\n","        else:\n","            print('\\\"{}\\\" is an invalid input! Please try again.\\n'.format(m))\n","\n","    # Choose keywords:\n","    if m == 'file':\n","        print('===========================================================')\n","        print('Please input the file name that contains your key words.')\n","        print('Notes:')\n","        print('    The file should contain key words in one or multiple lines, and multiple key words should be separated by *COMMA*.')\n","        print('        For example: NBA, basketball, Lebron James')\n","        print('    If the file is under the current directory, you can directly type the file name, e.g., \"keywords.txt\".')\n","        print('    If the file is in another directory, please type the full file name, e.g., \"C:\\\\Downloads\\\\keywords.txt\" (for Windows), or \"/Users/xy/Downloads/keywords.txt\" (for MacOS/Linux).')\n","\n","        while True:\n","            file_name = data_directory + input('Type your file name >>> ')\n","            # The line above is for reading an input file from data directory specified in the beginning of this function.\n","            if os.path.isfile(file_name):\n","                break\n","            else:\n","                print('\"{}\" is not a valid file name! Please check if the file exists.\\n'.format(file_name))\n","\n","        # Check the content of keywords file\n","        key_words = []\n","        with open(file_name, 'r') as f:\n","            lines = f.readlines()\n","            if len(lines) == 0:\n","                print('\\n{} is an empty file!\\nTask aborted!'.format(file_name))\n","                sys.exit(1)\n","\n","            for line in lines:\n","                line = line.strip()\n","                # Detect non-ASCII characters\n","                for c in line:\n","                    if ord(c) >= 128:\n","                        print('\\n{} contains non-ASCII characters: \"{}\" \\nPlease remove them and try again'.format(file_name, c))\n","                        sys.exit(1)\n","                # Check delimiters\n","                if line.count(' ') > 1 and ',' not in line:\n","                    print('\\nMore than 1 <space> symbols exist in the key words file, but none comma exists')\n","                    print('I\\'m confused about your keywords. Please separate your key words by commas.')\n","                    sys.exit(1)\n","\n","                words = line.split(',')\n","                for w in words:\n","                    if len(w.strip()) > 0:\n","                        key_words.append(w.strip())\n","\n","        # Check key_words\n","        if len(key_words) == 0:\n","            print('\\nZero key words are found in {}! Please check your key words file.'.format(file_name))\n","            sys.exit(1)\n","\n","    elif m == 'manual':\n","        print('===========================================================')\n","        print('Please input your key words (separated by comma), and hit <ENTER> when done.')\n","\n","        while True:\n","            line = input('Type the key words >>> ')\n","            line = line.strip()\n","\n","            invalid_flag = False\n","            # Check empty\n","            if len(line) == 0:\n","                print('\\nYour input is empty! Please try again.')\n","                invalid_flag = True\n","            # Detect non-ASCII characters\n","            for c in line:\n","                if ord(c) >= 128:\n","                    print('\\nYour input contains non-ASCII characters: \"{}\"! Please try again.'.format(c))\n","                    invalid_flag = True\n","                    break\n","            # Check delimiters\n","            if line.count(' ') > 1 and ',' not in line:\n","                print('\\nMore than 1 <space> symbols exist in your input, but none comma exists')\n","                print('I\\'m confused about your keywords. Please try again')\n","                invalid_flag = True\n","\n","            if invalid_flag:\n","                continue\n","            else:\n","                break\n","\n","        # Process input\n","        key_words = []\n","        for w in line.split(','):\n","            if len(w.strip()) > 0:\n","                key_words.append(w.strip())\n","\n","    # Print valid key words\n","    key_words = list(set(key_words))\n","    print('\\n{} unique key words being used: '.format(len(key_words)), key_words)\n","\n","    # Prompt for number of tweets to be gathered\n","    print('===========================================================')\n","    print('How many tweets do you want to gather? \\nInput an integer number, or just hit <ENTER> to use the default number 300.')\n","    num_tweets = 300\n","    while True:\n","        s = input('Input an integer >>> ')\n","        s = s.strip()\n","        if len(s) == 0:\n","            break\n","        elif s.isdigit():\n","            num = int(s)\n","            if num > 0:\n","                num_tweets = num\n","                break\n","            else:\n","                print('\\nPlease input a number that is greater than 0.')\n","        else:\n","            print('\\nPlease input a valid integer number.')\n","\n","    print('{} tweets to be gathered.'.format(num_tweets))\n","\n","    # Streaming\n","    # TODO: remvoe '\\t', '\\n' and ',' in text field, also remove empty text\n","    print('===========================================================')\n","    print('Start gathering tweets ...')\n","\n","    postfix = current_datetime_str()\n","    raw_file = 'raw_{}.json'.format(postfix)\n","    csv_file = 'data_{}.csv'.format(postfix)\n","    text_file = 'text_{}.csv'.format(postfix)\n","    \n","    # The data_directory is the directory in Google Drive, specified in the beginning, that the gathered Tweets will be stored\n","    raw_path = data_directory + raw_file\n","    csv_path = data_directory + csv_file\n","    text_path = data_directory + text_file\n","\n","    twitter_stream = Stream(auth, MyListener(raw_file=raw_path, csv_file=csv_path, text_file=text_path, max_num=num_tweets))\n","    twitter_stream.filter(track=key_words)\n","\n","\n","if __name__ == '__main__':\n","    try:\n","        main()\n","    except KeyboardInterrupt:\n","        print('\\nTask aborted!')\n","        \n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["===========================================================\n","Welcome to the user interface of gathering tweets pipeline!\n","You can press \"Ctrl+C\" at anytime to abort the program.\n","===========================================================\n","\n","How do you want to specify your key words?\n","Type \"manual\" or \"file\" >>> file\n","===========================================================\n","Please input the file name that contains your key words.\n","Notes:\n","    The file should contain key words in one or multiple lines, and multiple key words should be separated by *COMMA*.\n","        For example: NBA, basketball, Lebron James\n","    If the file is under the current directory, you can directly type the file name, e.g., \"keywords.txt\".\n","    If the file is in another directory, please type the full file name, e.g., \"C:\\Downloads\\keywords.txt\" (for Windows), or \"/Users/xy/Downloads/keywords.txt\" (for MacOS/Linux).\n","Type your file name >>> keyword.txt\n","\n","4 unique key words being used:  ['#trip', 'Trip', 'Travel', '#travel']\n","===========================================================\n","How many tweets do you want to gather? \n","Input an integer number, or just hit <ENTER> to use the default number 300.\n","Input an integer >>> 1000\n","1000 tweets to be gathered.\n","===========================================================\n","Start gathering tweets ...\n","1000/1000 tweets downloaded\n","Maximum number reached.\n","It took 386.2026674747467 seconds to download 1000 tweets\n"],"name":"stdout"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"_OPKatLFjE0d","executionInfo":{"status":"ok","timestamp":1601265295031,"user_tz":240,"elapsed":4403,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"579d4064-f622-4802-9d1a-1ac63774a37a","colab":{"base_uri":"https://localhost:8080/","height":802}},"source":["!pip install datascience"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: datascience in /usr/local/lib/python3.6/dist-packages (0.10.6)\n","Collecting folium==0.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/dd/75ced7437bfa7cb9a88b96ee0177953062803c3b4cde411a97d98c35adaf/folium-0.2.1.tar.gz (69kB)\n","\u001b[K     |████████████████████████████████| 71kB 4.4MB/s \n","\u001b[?25hRequirement already satisfied: sphinx in /usr/local/lib/python3.6/dist-packages (from datascience) (1.8.5)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from datascience) (3.6.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from datascience) (50.3.0)\n","Requirement already satisfied: coveralls==0.5 in /usr/local/lib/python3.6/dist-packages (from datascience) (0.5)\n","Requirement already satisfied: coverage==3.7.1 in /usr/local/lib/python3.6/dist-packages (from datascience) (3.7.1)\n","Requirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from folium==0.2.1->datascience) (2.11.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (1.15.0)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (2.8.0)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (1.2.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (20.4)\n","Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (0.15.2)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (2.6.1)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (1.2.0)\n","Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (2.23.0)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (2.0.0)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx->datascience) (0.7.12)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->datascience) (8.5.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->datascience) (0.7.1)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->datascience) (1.9.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->datascience) (1.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->datascience) (20.2.0)\n","Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from coveralls==0.5->datascience) (3.13)\n","Requirement already satisfied: docopt>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from coveralls==0.5->datascience) (0.6.2)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2->folium==0.2.1->datascience) (1.1.1)\n","Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.6/dist-packages (from babel!=2.0,>=1.3->sphinx->datascience) (2018.9)\n","Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.6/dist-packages (from sphinxcontrib-websupport->sphinx->datascience) (1.1.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->sphinx->datascience) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->datascience) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->datascience) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->datascience) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->sphinx->datascience) (2.10)\n","Building wheels for collected packages: folium\n","  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for folium: filename=folium-0.2.1-cp36-none-any.whl size=79980 sha256=86ccce8b8aa35c2e5f0738302d94a48a2bceda2fddcda306aa77864d33f33851\n","  Stored in directory: /root/.cache/pip/wheels/b8/09/f0/52d2ef419c2aaf4fb149f92a33e0008bdce7ae816f0dd8f0c5\n","Successfully built folium\n","Installing collected packages: folium\n","  Found existing installation: folium 0.8.3\n","    Uninstalling folium-0.8.3:\n","      Successfully uninstalled folium-0.8.3\n","Successfully installed folium-0.2.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"trusted":true,"id":"k5W9lvTIIb4m","executionInfo":{"status":"ok","timestamp":1601265298344,"user_tz":240,"elapsed":742,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"f70964ec-82c3-4d56-84cc-fdebf3be0825","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["from datascience import *"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/datascience/tables.py:17: MatplotlibDeprecationWarning: The 'warn' parameter of use() is deprecated since Matplotlib 3.1 and will be removed in 3.3.  If any parameter follows 'warn', they should be pass as keyword, not positionally.\n","  matplotlib.use('agg', warn=False)\n","/usr/local/lib/python3.6/dist-packages/datascience/util.py:10: MatplotlibDeprecationWarning: The 'warn' parameter of use() is deprecated since Matplotlib 3.1 and will be removed in 3.3.  If any parameter follows 'warn', they should be pass as keyword, not positionally.\n","  matplotlib.use('agg', warn=False)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MNOe8b1xugo5"},"source":["# Exercise 2 (5 points)\n","Run the code below to show the tweets file generated."]},{"cell_type":"code","metadata":{"id":"zaKA5MyfuV4D","executionInfo":{"status":"ok","timestamp":1601265308138,"user_tz":240,"elapsed":563,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"6b2602f6-0674-47bd-e0f0-6b94bbe5aecb","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!ls /content/drive/My\\ Drive/DS200Labs/Tweets/"],"execution_count":8,"outputs":[{"output_type":"stream","text":["data_2020-09-28_03-43-08.csv  raw_2020-09-28_03-43-08.json\n","keyword.txt\t\t      text_2020-09-28_03-43-08.csv\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f8eXwlljuECn"},"source":["# Exercise 3 (10 points)\n","\n","Find out the name of your tweets file that start with data. Replace XXXXXX in the path assignment below with the rest of the tweets data file. Read the tweets table, and sort it by \"text\", which is the body of tweets."]},{"cell_type":"code","metadata":{"id":"TKvSGmULsEnW","executionInfo":{"status":"ok","timestamp":1601265437845,"user_tz":240,"elapsed":488,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"35b23caf-5803-4503-9e55-758c1b86897b","colab":{"base_uri":"https://localhost:8080/","height":558}},"source":["path = \"/content/drive/My Drive/DS200Labs/Tweets/data_2020-09-28_03-43-08.csv\"\n","tweets_table = Table.read_table(path)\n","sorted_tweets = tweets_table.sort(\"text\")\n","sorted_tweets.show(10)"],"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","    <thead>\n","        <tr>\n","            <th>user_id</th> <th>user_name</th> <th>tweet_time</th> <th>location</th> <th>text</th>\n","        </tr>\n","    </thead>\n","    <tbody>\n","        <tr>\n","            <td>5674722            </td> <td>Davanum Srinivas                    </td> <td>Mon Sep 28 03:44:04 +0000 2020</td> <td>Massachusetts USA      </td> <td>#148 you are hilarious! I definitely enjoy your tweets e ...</td>\n","        </tr>\n","        <tr>\n","            <td>63791823           </td> <td>eVisitorGuide                       </td> <td>Mon Sep 28 03:49:03 +0000 2020</td> <td>CHI MKE STL & Nashville</td> <td>#Blues #brewpubs and some of the best #sportsbars in the ...</td>\n","        </tr>\n","        <tr>\n","            <td>727580305108938752 </td> <td>WomensPowerCen                      </td> <td>Mon Sep 28 03:44:03 +0000 2020</td> <td>None                   </td> <td>#MTVStars #coke Through food drinks clothes amenities &a ...</td>\n","        </tr>\n","        <tr>\n","            <td>437798374          </td> <td>BAL Immigration                     </td> <td>Mon Sep 28 03:45:03 +0000 2020</td> <td>None                   </td> <td>#Malaysia has relaxed its entry ban for some travelers.  ...</td>\n","        </tr>\n","        <tr>\n","            <td>1226094135431585792</td> <td>                                    </td> <td>Mon Sep 28 03:43:32 +0000 2020</td> <td>Kamagut                </td> <td>#UhuruNaKazi The new Naivasha- Njambini road is key to i ...</td>\n","        </tr>\n","        <tr>\n","            <td>954473252969205760 </td> <td>FBMyNextCar / WebPass Social Network</td> <td>Mon Sep 28 03:45:01 +0000 2020</td> <td>Florida USA            </td> <td>#WebPass #Florida @EdMorseMazdaPortRichey checkout Speci ...</td>\n","        </tr>\n","        <tr>\n","            <td>2907966134         </td> <td>shabnam roy                         </td> <td>Mon Sep 28 03:47:03 +0000 2020</td> <td>Mumbai India           </td> <td>#cleartrip I have been hearing this for over 6 months no ...</td>\n","        </tr>\n","        <tr>\n","            <td>1130398556836306944</td> <td>Styles Mix                          </td> <td>Mon Sep 28 03:46:27 +0000 2020</td> <td>None                   </td> <td>#deluxe #luxury #design Portable Comfortable Reversible  ...</td>\n","        </tr>\n","        <tr>\n","            <td>617853906          </td> <td>Brett Murphy                        </td> <td>Mon Sep 28 03:45:12 +0000 2020</td> <td>Bay Area CA            </td> <td>#selfemployed #network #makemoney #affiliateprogram Chec ...</td>\n","        </tr>\n","        <tr>\n","            <td>1087583509005418496</td> <td>4 Cycling Store                     </td> <td>Mon Sep 28 03:49:11 +0000 2020</td> <td>None                   </td> <td>#yogini #travel Universal Breathable Cycling Headbandhtt ...</td>\n","        </tr>\n","    </tbody>\n","</table>\n","<p>... (994 rows omitted)</p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"GggUjk8cbl5h"},"source":["# Counting Words or Hashtags in Tweets\n","\n","Suppose we are interested in having a high-level view about the frequency that a specific word or a hashtag (e.g., COVID, #Covid or #Covid19) occurs in each tweet. We can do this in a few steps:\n","- First, convert the string of text (in each tweet) into a list of words using the Python function split.\n","- Then, count, for each tweet, how many times the word or hashtag you are interested in occurs in the list.  \n","- Third, add the count for each word or hashtag to obtain a total.\n","\n","The following Python code demonstrates how split converts a string representation of text into a list of words, which enable further processing of the text."]},{"cell_type":"code","metadata":{"id":"cKm-PrkGz8aa","executionInfo":{"status":"ok","timestamp":1601265569914,"user_tz":240,"elapsed":581,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"b2451d22-a36a-4425-fcec-e6423300bf58","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["String = \"This is a tweet about tweet, but not a tweet about Covid-19. I don't think it matters, though. #COVID19\"\n","BoW_list = String.split(\" \")\n","print(BoW_list)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["['This', 'is', 'a', 'tweet', 'about', 'tweet,', 'but', 'not', 'a', 'tweet', 'about', 'Covid-19.', 'I', \"don't\", 'think', 'it', 'matters,', 'though.', '#COVID19']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_np-LDfd2WIc","executionInfo":{"status":"ok","timestamp":1601265945716,"user_tz":240,"elapsed":483,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"78bdd51f-1416-433b-8e3c-82ab048dc11d","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["'tweet' in BoW_list"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"p6iCGd441GMm"},"source":["# Exercise 4 (5 points)\n","- (a) Complete the following code so that split uses period \".\" (not sapce \" \") as the delimeter for splitting a given string. \n","- (b) Discuss the difference between using period \".\" versus \" \" in splitting a string. "]},{"cell_type":"code","metadata":{"id":"LCysjGrs2nPU","executionInfo":{"status":"ok","timestamp":1601266361482,"user_tz":240,"elapsed":576,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"d3bc6960-f1dd-4394-8bec-524ec503b9af","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Exercise 4 (a)\n","list2 = String.split(\".\")\n","print(list2)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["['This is a tweet about tweet, but not a tweet about Covid-19', \" I don't think it matters, though\", ' #COVID19']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"F3UhBsdK20xW"},"source":["# Answer to Exercise 4 (b):\n","\n","Using period to split the string, we can get a full sentence, while using space, the string will split word by word."]},{"cell_type":"code","metadata":{"id":"7vRky6-3tYZ-","executionInfo":{"status":"ok","timestamp":1601266426634,"user_tz":240,"elapsed":480,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}}},"source":["tweets_BoW = sorted_tweets.apply(lambda x: x.split(' '), \"text\") "],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"0W8gcJ85f-H9","executionInfo":{"status":"ok","timestamp":1601266428032,"user_tz":240,"elapsed":480,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"16b01fa0-746a-45c8-a858-51cb594b604e","colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["print(tweets_BoW)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[list(['#148', 'you', 'are', 'hilarious!', 'I', 'definitely', 'enjoy', 'your', 'tweets', 'especially', 'about', 'food/travel', 'multi', 'cultural', 'stuff.', 'Just', 'so', 'y', 'https://t.co/IQTvIL6Fve'])\n"," list(['#Blues', '#brewpubs', 'and', 'some', 'of', 'the', 'best', '#sportsbars', 'in', 'the', 'country!', 'Use', 'our', 'guide', 'to', 'plan', 'your', 'perfect', 'night', 'in', 'https://t.co/PfOL1tVGlP'])\n"," list(['#MTVStars', '#coke', 'Through', 'food', 'drinks', 'clothes', 'amenities', '&amp;', 'travel', 'we', 'enjoy', 'morethan', 'historys', '#emperors', 'We', 'still', 'grumble', 'https://t.co/J0Sno9FP6y'])\n"," ...\n"," list(['thats', 'exactly', 'what', 'it', 'sounds', 'like', 'cus', 'i', 'miss', 'u', 'biTCH'])\n"," list(['this', 'is', 'me', 'subtweeting', '@engelicaraya', '....', 'the', 'group', 'needs', 'a', 'beach', 'trip', '....'])\n"," list(['tl;dr', 'they', 'say', 'youll', 'still', 'have', 'to', 'take', 'off', 'your', 'shoes', 'at', 'tsa'])]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xjUkV-h5gIPk","executionInfo":{"status":"ok","timestamp":1601266431413,"user_tz":240,"elapsed":801,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"3cb39edb-643f-4d54-de63-e0e43020c235","colab":{"base_uri":"https://localhost:8080/","height":318}},"source":["TweetsTable_BoW=sorted_tweets.with_column('BoW', tweets_BoW)\n","TweetsTable_BoW.show(5)"],"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","    <thead>\n","        <tr>\n","            <th>user_id</th> <th>user_name</th> <th>tweet_time</th> <th>location</th> <th>text</th> <th>BoW</th>\n","        </tr>\n","    </thead>\n","    <tbody>\n","        <tr>\n","            <td>5674722            </td> <td>Davanum Srinivas</td> <td>Mon Sep 28 03:44:04 +0000 2020</td> <td>Massachusetts USA      </td> <td>#148 you are hilarious! I definitely enjoy your tweets e ...</td> <td>['#148', 'you', 'are', 'hilarious!', 'I', 'definitely',  ...</td>\n","        </tr>\n","        <tr>\n","            <td>63791823           </td> <td>eVisitorGuide   </td> <td>Mon Sep 28 03:49:03 +0000 2020</td> <td>CHI MKE STL & Nashville</td> <td>#Blues #brewpubs and some of the best #sportsbars in the ...</td> <td>['#Blues', '#brewpubs', 'and', 'some', 'of', 'the', 'bes ...</td>\n","        </tr>\n","        <tr>\n","            <td>727580305108938752 </td> <td>WomensPowerCen  </td> <td>Mon Sep 28 03:44:03 +0000 2020</td> <td>None                   </td> <td>#MTVStars #coke Through food drinks clothes amenities &a ...</td> <td>['#MTVStars', '#coke', 'Through', 'food', 'drinks', 'clo ...</td>\n","        </tr>\n","        <tr>\n","            <td>437798374          </td> <td>BAL Immigration </td> <td>Mon Sep 28 03:45:03 +0000 2020</td> <td>None                   </td> <td>#Malaysia has relaxed its entry ban for some travelers.  ...</td> <td>['#Malaysia', 'has', 'relaxed', 'its', 'entry', 'ban', ' ...</td>\n","        </tr>\n","        <tr>\n","            <td>1226094135431585792</td> <td>                </td> <td>Mon Sep 28 03:43:32 +0000 2020</td> <td>Kamagut                </td> <td>#UhuruNaKazi The new Naivasha- Njambini road is key to i ...</td> <td>['#UhuruNaKazi', 'The', 'new', 'Naivasha-', 'Njambini',  ...</td>\n","        </tr>\n","    </tbody>\n","</table>\n","<p>... (999 rows omitted)</p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"H3hHXZJBmSC5"},"source":["# Using .COUNT of List to Compute Word Frequency\n","\n","We saw the output of applying .split(' ') to a string is a list of words/terms in the string (which we also refer to as \"a Bag of Word\").  \n","\n","In the following exercise, we are going to use .count method of a list to count how many times a word occurs in the list. For example, "]},{"cell_type":"code","metadata":{"id":"9Wjzi6bMnrIW","executionInfo":{"status":"ok","timestamp":1601266399086,"user_tz":240,"elapsed":1014,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"d0ffd03d-b2ff-4010-b5e9-1dfc16f16f35","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["String = \"This is a tweet about tweet, but not a tweet about Covid-19. #COVID19\"\n","BoW_list = String.split(\" \")\n","print(BoW_list)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["['This', 'is', 'a', 'tweet', 'about', 'tweet,', 'but', 'not', 'a', 'tweet', 'about', 'Covid-19.', '#COVID19']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LXxJwLm5oIce","executionInfo":{"status":"ok","timestamp":1601266450000,"user_tz":240,"elapsed":483,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"35279397-2a67-4e0e-c73d-c674c33f4d05","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# This code returns a number that indicates how many time the list (BoW_list) contains the word \"tweet\"\n","BoW_list.count('tweet')"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"4YWTFSlzpbu4","executionInfo":{"status":"ok","timestamp":1601266451833,"user_tz":240,"elapsed":551,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"ff0005cf-7752-4486-9674-8cb9868f5115","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["BoW_list.count('#Covid19')"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"ivb0LfPdpijD","executionInfo":{"status":"ok","timestamp":1601266453831,"user_tz":240,"elapsed":459,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"057061b6-2f75-405f-cdba-074cf0a5961c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["BoW_list.count(\"#COVID19\")"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"pWj_S9RQl1_j"},"source":["# Exercise 5 (10 points)\n","Select a word or a hashtag you used to sample the tweets, complete the following code for both (a) and (b):\n","- (a) Determine, for each tweet, the frequency your chosen word (or hashtag) occur in the tweets you gathered using .count method to the \"BoW\" column of the table TweetsTable_Bow using .COUNT.\n","- (b) Add the count for each tweet to the table TweetsTable_BoW as a new column.  You can choose the name of the column based on the name of the word or hashtag you used.\n"," "]},{"cell_type":"code","metadata":{"id":"9Am-iT0Ug6-6","executionInfo":{"status":"ok","timestamp":1601267440060,"user_tz":240,"elapsed":455,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"00b3e2af-1bbe-4af0-d100-d47d904f5da8","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["travel_count = TweetsTable_BoW.apply(lambda x: x.count('travel'), \"BoW\")\n","print(travel_count)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["[0 0 1 ... 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kE-R_CGPkFXZ","executionInfo":{"status":"ok","timestamp":1601267442294,"user_tz":240,"elapsed":762,"user":{"displayName":"Haichen Wei","photoUrl":"","userId":"02953937478055381848"}},"outputId":"4b04df5b-1982-40bd-8259-9a4b24b13479","colab":{"base_uri":"https://localhost:8080/","height":728}},"source":["Tweets_Table_BoW_Count = TweetsTable_BoW.with_columns(\"travel_count\", travel_count)\n","Tweets_Table_BoW_Count.show(10)"],"execution_count":35,"outputs":[{"output_type":"display_data","data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","    <thead>\n","        <tr>\n","            <th>user_id</th> <th>user_name</th> <th>tweet_time</th> <th>location</th> <th>text</th> <th>BoW</th> <th>travel_count</th>\n","        </tr>\n","    </thead>\n","    <tbody>\n","        <tr>\n","            <td>5674722            </td> <td>Davanum Srinivas                    </td> <td>Mon Sep 28 03:44:04 +0000 2020</td> <td>Massachusetts USA      </td> <td>#148 you are hilarious! I definitely enjoy your tweets e ...</td> <td>['#148', 'you', 'are', 'hilarious!', 'I', 'definitely',  ...</td> <td>0           </td>\n","        </tr>\n","        <tr>\n","            <td>63791823           </td> <td>eVisitorGuide                       </td> <td>Mon Sep 28 03:49:03 +0000 2020</td> <td>CHI MKE STL & Nashville</td> <td>#Blues #brewpubs and some of the best #sportsbars in the ...</td> <td>['#Blues', '#brewpubs', 'and', 'some', 'of', 'the', 'bes ...</td> <td>0           </td>\n","        </tr>\n","        <tr>\n","            <td>727580305108938752 </td> <td>WomensPowerCen                      </td> <td>Mon Sep 28 03:44:03 +0000 2020</td> <td>None                   </td> <td>#MTVStars #coke Through food drinks clothes amenities &a ...</td> <td>['#MTVStars', '#coke', 'Through', 'food', 'drinks', 'clo ...</td> <td>1           </td>\n","        </tr>\n","        <tr>\n","            <td>437798374          </td> <td>BAL Immigration                     </td> <td>Mon Sep 28 03:45:03 +0000 2020</td> <td>None                   </td> <td>#Malaysia has relaxed its entry ban for some travelers.  ...</td> <td>['#Malaysia', 'has', 'relaxed', 'its', 'entry', 'ban', ' ...</td> <td>0           </td>\n","        </tr>\n","        <tr>\n","            <td>1226094135431585792</td> <td>                                    </td> <td>Mon Sep 28 03:43:32 +0000 2020</td> <td>Kamagut                </td> <td>#UhuruNaKazi The new Naivasha- Njambini road is key to i ...</td> <td>['#UhuruNaKazi', 'The', 'new', 'Naivasha-', 'Njambini',  ...</td> <td>0           </td>\n","        </tr>\n","        <tr>\n","            <td>954473252969205760 </td> <td>FBMyNextCar / WebPass Social Network</td> <td>Mon Sep 28 03:45:01 +0000 2020</td> <td>Florida USA            </td> <td>#WebPass #Florida @EdMorseMazdaPortRichey checkout Speci ...</td> <td>['#WebPass', '#Florida', '@EdMorseMazdaPortRichey', 'che ...</td> <td>0           </td>\n","        </tr>\n","        <tr>\n","            <td>2907966134         </td> <td>shabnam roy                         </td> <td>Mon Sep 28 03:47:03 +0000 2020</td> <td>Mumbai India           </td> <td>#cleartrip I have been hearing this for over 6 months no ...</td> <td>['#cleartrip', 'I', 'have', 'been', 'hearing', 'this', ' ...</td> <td>0           </td>\n","        </tr>\n","        <tr>\n","            <td>1130398556836306944</td> <td>Styles Mix                          </td> <td>Mon Sep 28 03:46:27 +0000 2020</td> <td>None                   </td> <td>#deluxe #luxury #design Portable Comfortable Reversible  ...</td> <td>['#deluxe', '#luxury', '#design', 'Portable', 'Comfortab ...</td> <td>0           </td>\n","        </tr>\n","        <tr>\n","            <td>617853906          </td> <td>Brett Murphy                        </td> <td>Mon Sep 28 03:45:12 +0000 2020</td> <td>Bay Area CA            </td> <td>#selfemployed #network #makemoney #affiliateprogram Chec ...</td> <td>['#selfemployed', '#network', '#makemoney', '#affiliatep ...</td> <td>0           </td>\n","        </tr>\n","        <tr>\n","            <td>1087583509005418496</td> <td>4 Cycling Store                     </td> <td>Mon Sep 28 03:49:11 +0000 2020</td> <td>None                   </td> <td>#yogini #travel Universal Breathable Cycling Headbandhtt ...</td> <td>['#yogini', '#travel', 'Universal', 'Breathable', 'Cycli ...</td> <td>0           </td>\n","        </tr>\n","    </tbody>\n","</table>\n","<p>... (994 rows omitted)</p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]}]}